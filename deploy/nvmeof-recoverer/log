2024/05/19 17:29:11 maxprocs: Leaving GOMAXPROCS=4: CPU quota undefined
2024-05-19 17:29:11.998268 I | rookcmd: starting Rook 1.2.3 with arguments '/usr/local/bin/rook ceph operator'
2024-05-19 17:29:11.998377 I | rookcmd: flag values: --enable-machine-disruption-budget=false, --help=false, --kubeconfig=, --log-level=INFO
2024-05-19 17:29:11.998380 I | cephcmd: starting Rook-Ceph operator
2024-05-19 17:29:12.314151 I | cephcmd: base ceph version inside the rook operator image is "ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)"
2024-05-19 17:29:12.382879 I | op-k8sutil: ROOK_CURRENT_NAMESPACE_ONLY="false" (env var)
2024-05-19 17:29:12.382913 I | operator: watching all namespaces for Ceph CRs
2024-05-19 17:29:12.382969 I | operator: setting up schemes
2024-05-19 17:29:12.384332 I | operator: setting up the controller-runtime manager
2024-05-19 17:29:12.384789 I | ceph-cluster-controller: successfully started
2024-05-19 17:29:12.386722 I | op-k8sutil: ROOK_DISABLE_DEVICE_HOTPLUG="false" (configmap)
2024-05-19 17:29:12.386759 I | ceph-cluster-controller: enabling hotplug orchestration
2024-05-19 17:29:12.386846 I | ceph-nodedaemon-controller: successfully started
2024-05-19 17:29:12.386965 I | ceph-block-pool-controller: successfully started
2024-05-19 17:29:12.387076 I | ceph-object-store-user-controller: successfully started
2024-05-19 17:29:12.387100 I | ceph-object-realm-controller: successfully started
2024-05-19 17:29:12.387168 I | ceph-object-zonegroup-controller: successfully started
2024-05-19 17:29:12.387177 I | ceph-object-zone-controller: successfully started
2024-05-19 17:29:12.387290 I | ceph-object-controller: successfully started
2024-05-19 17:29:12.387353 I | ceph-file-controller: successfully started
2024-05-19 17:29:12.387369 I | ceph-nfs-controller: successfully started
2024-05-19 17:29:12.387376 I | ceph-rbd-mirror-controller: successfully started
2024-05-19 17:29:12.387442 I | ceph-client-controller: successfully started
2024-05-19 17:29:12.387457 I | ceph-filesystem-mirror-controller: successfully started
2024-05-19 17:29:12.387509 I | operator: rook-ceph-operator-config-controller successfully started
2024-05-19 17:29:12.387518 I | ceph-csi: rook-ceph-operator-csi-controller successfully started
2024-05-19 17:29:12.387598 I | op-bucket-prov: rook-ceph-operator-bucket-controller successfully started
2024-05-19 17:29:12.387617 I | ceph-bucket-topic: successfully started
2024-05-19 17:29:12.387621 I | ceph-bucket-notification: successfully started
2024-05-19 17:29:12.387624 I | ceph-bucket-notification: successfully started
2024-05-19 17:29:12.387754 I | ceph-fs-subvolumegroup-controller: successfully started
2024-05-19 17:29:12.387770 I | blockpool-rados-namespace-controller: successfully started
2024-05-19 17:29:12.387833 I | ceph-cosi-controller: successfully started
2024-05-19 17:29:12.387848 I | nvmeofstorage-controller: successfully started
2024-05-19 17:29:12.387929 I | nvmeofosd-controller: successfully started
2024-05-19 17:29:12.387950 I | operator: starting the controller-runtime manager
2024-05-19 17:29:12.518847 I | op-k8sutil: ROOK_WATCH_FOR_NODE_FAILURE="true" (configmap)
2024-05-19 17:29:12.715544 I | op-k8sutil: ROOK_CEPH_COMMANDS_TIMEOUT_SECONDS="15" (configmap)
2024-05-19 17:29:12.716017 I | op-k8sutil: ROOK_LOG_LEVEL="DEBUG" (configmap)
2024-05-19 17:29:12.716101 I | op-k8sutil: ROOK_ENABLE_DISCOVERY_DAEMON="false" (configmap)
2024-05-19 17:29:12.716191 I | ceph-spec: adding finalizer "cephblockpool.ceph.rook.io" on "builtin-mgr"
2024-05-19 17:29:12.716940 I | ceph-spec: adding finalizer "cephcluster.ceph.rook.io" on "my-cluster"
2024-05-19 17:29:12.739056 I | op-k8sutil: ROOK_CEPH_ALLOW_LOOP_DEVICES="false" (configmap)
2024-05-19 17:29:12.739076 I | operator: rook-ceph-operator-config-controller done reconciling
2024-05-19 17:29:12.739380 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: kube-system, Request.Name: kube-controller-manager-minikube
2024-05-19 17:29:12.739485 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace kube-system
2024-05-19 17:29:12.739495 E | nvmeofosd-controller: other status: Running
2024-05-19 17:29:12.739512 D | nvmeofosd-controller: successfully configured Pod "kube-system/kube-controller-manager-minikube"
2024-05-19 17:29:12.739679 D | ceph-nodedaemon-controller: reconciling node: "minikube"
2024-05-19 17:29:12.739719 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: kube-system, Request.Name: kube-proxy-gjhfx
2024-05-19 17:29:12.739782 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace kube-system
2024-05-19 17:29:12.739790 E | nvmeofosd-controller: other status: Running
2024-05-19 17:29:12.739808 D | nvmeofosd-controller: successfully configured Pod "kube-system/kube-proxy-gjhfx"
2024-05-19 17:29:12.739830 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: kube-system, Request.Name: storage-provisioner
2024-05-19 17:29:12.739862 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace kube-system
2024-05-19 17:29:12.739869 E | nvmeofosd-controller: other status: Running
2024-05-19 17:29:12.739877 D | nvmeofosd-controller: successfully configured Pod "kube-system/storage-provisioner"
2024-05-19 17:29:12.739888 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: kube-system, Request.Name: coredns-7db6d8ff4d-vxps6
2024-05-19 17:29:12.739927 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace kube-system
2024-05-19 17:29:12.739934 E | nvmeofosd-controller: other status: Running
2024-05-19 17:29:12.739961 D | nvmeofosd-controller: successfully configured Pod "kube-system/coredns-7db6d8ff4d-vxps6"
2024-05-19 17:29:12.739977 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: kube-system, Request.Name: coredns-7db6d8ff4d-wzhwk
2024-05-19 17:29:12.740025 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace kube-system
2024-05-19 17:29:12.740031 E | nvmeofosd-controller: other status: Running
2024-05-19 17:29:12.740046 D | nvmeofosd-controller: successfully configured Pod "kube-system/coredns-7db6d8ff4d-wzhwk"
2024-05-19 17:29:12.740062 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: kube-system, Request.Name: kube-apiserver-minikube
2024-05-19 17:29:12.740095 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace kube-system
2024-05-19 17:29:12.740102 E | nvmeofosd-controller: other status: Running
2024-05-19 17:29:12.740142 D | nvmeofosd-controller: successfully configured Pod "kube-system/kube-apiserver-minikube"
2024-05-19 17:29:12.740181 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-tools-67bf494bc8-pfzjm
2024-05-19 17:29:12.740229 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:29:12.740236 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:29:12.740261 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-tools-67bf494bc8-pfzjm"
2024-05-19 17:29:12.740280 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: kube-system, Request.Name: etcd-minikube
2024-05-19 17:29:12.740349 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace kube-system
2024-05-19 17:29:12.740357 E | nvmeofosd-controller: other status: Running
2024-05-19 17:29:12.740381 D | nvmeofosd-controller: successfully configured Pod "kube-system/etcd-minikube"
2024-05-19 17:29:12.740413 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: kube-system, Request.Name: kube-scheduler-minikube
2024-05-19 17:29:12.740449 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace kube-system
2024-05-19 17:29:12.740457 E | nvmeofosd-controller: other status: Running
2024-05-19 17:29:12.740468 D | nvmeofosd-controller: successfully configured Pod "kube-system/kube-scheduler-minikube"
2024-05-19 17:29:12.740485 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-operator-6f6bdb568c-4p4tv
2024-05-19 17:29:12.740527 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:29:12.740535 E | nvmeofosd-controller: other status: Running
2024-05-19 17:29:12.740569 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-operator-6f6bdb568c-4p4tv"
2024-05-19 17:29:12.741158 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-19 17:29:12.745938 D | ceph-spec: update event from a CR: "builtin-mgr"
2024-05-19 17:29:12.745965 D | ceph-spec: update event on CephBlockPool CR
2024-05-19 17:29:12.746022 D | ceph-spec: skipping resource "builtin-mgr" update with unchanged spec
2024-05-19 17:29:12.748743 I | clusterdisruption-controller: deleted all legacy node drain canary pods
2024-05-19 17:29:12.748757 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:29:12.748812 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:29:12.756206 W | ceph-block-pool-controller: failed to set pool "builtin-mgr" status to "Progressing". failed to update object "rook-ceph/builtin-mgr" status: Operation cannot be fulfilled on cephblockpools.ceph.rook.io "builtin-mgr": the object has been modified; please apply your changes to the latest version and try again
2024-05-19 17:29:12.756281 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-19 17:29:12.756291 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-19 17:29:12.756299 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-19 17:29:12.757243 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2024-05-19 17:29:12.757634 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-19 17:29:12.757651 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-19 17:29:12.758061 D | ceph-cluster-controller: skipping resource "my-cluster" update with unchanged spec
2024-05-19 17:29:12.775993 I | ceph-cluster-controller: clusterInfo not yet found, must be a new cluster.
2024-05-19 17:29:12.776414 I | ceph-csi: successfully created csi config map "rook-ceph-csi-config"
2024-05-19 17:29:12.776539 I | op-k8sutil: ROOK_CSI_DISABLE_DRIVER="false" (configmap)
2024-05-19 17:29:12.776810 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:29:12.799190 D | ceph-cluster-controller: cluster spec successfully validated
2024-05-19 17:29:12.799296 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Detecting Ceph version"
2024-05-19 17:29:12.859897 I | op-k8sutil: CSI_ENABLE_HOST_NETWORK="true" (default)
2024-05-19 17:29:12.859977 I | op-k8sutil: CSI_DISABLE_HOLDER_PODS="true" (configmap)
2024-05-19 17:29:12.861952 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v18...
2024-05-19 17:29:12.862801 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-19 17:29:12.862816 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-19 17:29:12.920333 D | op-k8sutil: ConfigMap rook-ceph-detect-version is already deleted
2024-05-19 17:29:12.922161 I | ceph-csi: cluster info for cluster "my-cluster" is not ready yet, will retry in 10s, proceeding with ready clusters
2024-05-19 17:29:12.922187 I | op-k8sutil: ROOK_CSI_ENABLE_RBD="true" (configmap)
2024-05-19 17:29:12.922190 I | op-k8sutil: ROOK_CSI_ENABLE_CEPHFS="true" (configmap)
2024-05-19 17:29:12.922192 I | op-k8sutil: ROOK_CSI_ENABLE_NFS="false" (configmap)
2024-05-19 17:29:12.922207 I | op-k8sutil: ROOK_CSI_ALLOW_UNSUPPORTED_VERSION="false" (configmap)
2024-05-19 17:29:12.922230 I | op-k8sutil: CSI_FORCE_CEPHFS_KERNEL_CLIENT="true" (configmap)
2024-05-19 17:29:12.922239 I | op-k8sutil: CSI_GRPC_TIMEOUT_SECONDS="150" (configmap)
2024-05-19 17:29:12.922251 I | op-k8sutil: CSI_CEPHFS_LIVENESS_METRICS_PORT="9081" (default)
2024-05-19 17:29:12.922258 I | op-k8sutil: CSIADDONS_PORT="9070" (default)
2024-05-19 17:29:12.922261 I | op-k8sutil: CSI_RBD_LIVENESS_METRICS_PORT="9080" (default)
2024-05-19 17:29:12.922263 I | op-k8sutil: CSI_ENABLE_LIVENESS="false" (configmap)
2024-05-19 17:29:12.922265 I | op-k8sutil: CSI_PLUGIN_PRIORITY_CLASSNAME="system-node-critical" (configmap)
2024-05-19 17:29:12.922268 I | op-k8sutil: CSI_PROVISIONER_PRIORITY_CLASSNAME="system-cluster-critical" (configmap)
2024-05-19 17:29:12.922274 I | op-k8sutil: CSI_ENABLE_OMAP_GENERATOR="false" (default)
2024-05-19 17:29:12.922276 I | op-k8sutil: CSI_ENABLE_RBD_SNAPSHOTTER="true" (configmap)
2024-05-19 17:29:12.922278 I | op-k8sutil: CSI_ENABLE_CEPHFS_SNAPSHOTTER="true" (configmap)
2024-05-19 17:29:12.922280 I | op-k8sutil: CSI_ENABLE_NFS_SNAPSHOTTER="true" (configmap)
2024-05-19 17:29:12.924125 I | op-k8sutil: CSI_ENABLE_CSIADDONS="false" (configmap)
2024-05-19 17:29:12.924143 I | op-k8sutil: CSI_ENABLE_TOPOLOGY="false" (configmap)
2024-05-19 17:29:12.924146 I | op-k8sutil: CSI_ENABLE_ENCRYPTION="false" (configmap)
2024-05-19 17:29:12.924149 I | op-k8sutil: CSI_ENABLE_METADATA="false" (default)
2024-05-19 17:29:12.924153 I | op-k8sutil: CSI_CEPHFS_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2024-05-19 17:29:12.924155 I | op-k8sutil: CSI_CEPHFS_PLUGIN_UPDATE_STRATEGY_MAX_UNAVAILABLE="1" (default)
2024-05-19 17:29:12.924157 I | op-k8sutil: CSI_NFS_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2024-05-19 17:29:12.924159 I | op-k8sutil: CSI_RBD_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2024-05-19 17:29:12.924160 I | op-k8sutil: CSI_RBD_PLUGIN_UPDATE_STRATEGY_MAX_UNAVAILABLE="1" (default)
2024-05-19 17:29:12.924162 I | op-k8sutil: CSI_PLUGIN_ENABLE_SELINUX_HOST_MOUNT="false" (configmap)
2024-05-19 17:29:12.924164 I | ceph-csi: Kubernetes version is 1.30
2024-05-19 17:29:12.924166 I | op-k8sutil: CSI_LOG_LEVEL="" (default)
2024-05-19 17:29:12.924168 I | op-k8sutil: CSI_SIDECAR_LOG_LEVEL="" (default)
2024-05-19 17:29:12.924169 I | op-k8sutil: CSI_LEADER_ELECTION_LEASE_DURATION="" (default)
2024-05-19 17:29:12.924171 I | op-k8sutil: CSI_LEADER_ELECTION_RENEW_DEADLINE="" (default)
2024-05-19 17:29:12.924172 I | op-k8sutil: CSI_LEADER_ELECTION_RETRY_PERIOD="" (default)
2024-05-19 17:29:13.015669 I | op-k8sutil: ROOK_CSI_CEPH_IMAGE="quay.io/cephcsi/cephcsi:v3.11.0" (default)
2024-05-19 17:29:13.021441 I | op-k8sutil: ROOK_CSI_REGISTRAR_IMAGE="registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.10.1" (default)
2024-05-19 17:29:13.021723 I | op-k8sutil: ROOK_CSI_PROVISIONER_IMAGE="registry.k8s.io/sig-storage/csi-provisioner:v4.0.1" (default)
2024-05-19 17:29:13.021747 I | op-k8sutil: ROOK_CSI_ATTACHER_IMAGE="registry.k8s.io/sig-storage/csi-attacher:v4.5.1" (default)
2024-05-19 17:29:13.021751 I | op-k8sutil: ROOK_CSI_SNAPSHOTTER_IMAGE="registry.k8s.io/sig-storage/csi-snapshotter:v7.0.2" (default)
2024-05-19 17:29:13.021771 I | op-k8sutil: ROOK_CSI_RESIZER_IMAGE="registry.k8s.io/sig-storage/csi-resizer:v1.10.1" (default)
2024-05-19 17:29:13.021774 I | op-k8sutil: ROOK_CSI_KUBELET_DIR_PATH="/var/lib/kubelet" (default)
2024-05-19 17:29:13.021811 I | op-k8sutil: ROOK_CSIADDONS_IMAGE="quay.io/csiaddons/k8s-sidecar:v0.8.0" (default)
2024-05-19 17:29:13.021814 I | op-k8sutil: CSI_TOPOLOGY_DOMAIN_LABELS="" (default)
2024-05-19 17:29:13.021817 I | op-k8sutil: ROOK_CSI_CEPHFS_POD_LABELS="" (default)
2024-05-19 17:29:13.021836 I | op-k8sutil: ROOK_CSI_NFS_POD_LABELS="" (default)
2024-05-19 17:29:13.021920 I | op-k8sutil: ROOK_CSI_RBD_POD_LABELS="" (default)
2024-05-19 17:29:13.021970 I | op-k8sutil: CSI_CLUSTER_NAME="" (default)
2024-05-19 17:29:13.021997 I | op-k8sutil: ROOK_CSI_IMAGE_PULL_POLICY="IfNotPresent" (default)
2024-05-19 17:29:13.022001 I | op-k8sutil: CSI_CEPHFS_KERNEL_MOUNT_OPTIONS="" (default)
2024-05-19 17:29:13.022040 I | op-k8sutil: CSI_CEPHFS_ATTACH_REQUIRED="true" (configmap)
2024-05-19 17:29:13.022070 I | op-k8sutil: CSI_RBD_ATTACH_REQUIRED="true" (configmap)
2024-05-19 17:29:13.022267 I | op-k8sutil: CSI_NFS_ATTACH_REQUIRED="true" (configmap)
2024-05-19 17:29:13.022296 I | op-k8sutil: CSI_DRIVER_NAME_PREFIX="rook-ceph" (default)
2024-05-19 17:29:13.030198 I | op-k8sutil: CSI_ENABLE_VOLUME_GROUP_SNAPSHOT="true" (configmap)
2024-05-19 17:29:13.030221 I | ceph-csi: detecting the ceph csi image version for image "quay.io/cephcsi/cephcsi:v3.11.0"
2024-05-19 17:29:13.030270 I | op-k8sutil: CSI_PROVISIONER_TOLERATIONS="" (default)
2024-05-19 17:29:13.030278 I | op-k8sutil: CSI_PROVISIONER_NODE_AFFINITY="" (default)
2024-05-19 17:29:13.030315 D | ceph-spec: create event from a CR: "rook-ceph-detect-version-2lnnf"
2024-05-19 17:29:13.030377 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-detect-version-2lnnf
2024-05-19 17:29:13.030443 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:29:13.030451 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:29:13.030458 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-detect-version-2lnnf"
2024-05-19 17:29:13.037066 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-19 17:29:13.037295 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-19 17:29:13.037366 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:29:13.037394 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:29:13.037470 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:29:13.049796 D | ceph-spec: update event from a CR: "rook-ceph-detect-version-2lnnf"
2024-05-19 17:29:13.051260 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-detect-version-2lnnf
2024-05-19 17:29:13.051279 D | ceph-spec: wjkim : objNew.Name %srook-ceph-detect-version-2lnnf
2024-05-19 17:29:13.051281 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:29:13.083052 D | ceph-spec: update event from a CR: "rook-ceph-detect-version-2lnnf"
2024-05-19 17:29:13.083162 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-detect-version-2lnnf
2024-05-19 17:29:13.083176 D | ceph-spec: wjkim : objNew.Name %srook-ceph-detect-version-2lnnf
2024-05-19 17:29:13.083185 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:29:13.159999 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-19 17:29:13.160416 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-19 17:29:13.160533 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:29:13.160610 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:29:13.160646 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:29:13.437095 D | op-k8sutil: ConfigMap rook-ceph-csi-detect-version is already deleted
2024-05-19 17:29:13.604962 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-19 17:29:13.675502 D | ceph-spec: create event from a CR: "rook-ceph-csi-detect-version-5nhh7"
2024-05-19 17:29:13.675712 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:29:13.675847 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:29:13.675861 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:29:13.675872 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-csi-detect-version-5nhh7"
2024-05-19 17:29:13.699357 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-5nhh7"
2024-05-19 17:29:13.699565 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:29:13.699630 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-5nhh7
2024-05-19 17:29:13.699648 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:29:13.737458 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-5nhh7"
2024-05-19 17:29:13.737759 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:29:13.737788 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-5nhh7
2024-05-19 17:29:13.737796 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:29:16.018501 D | ceph-spec: update event from a CR: "rook-ceph-detect-version-2lnnf"
2024-05-19 17:29:16.020712 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-detect-version-2lnnf
2024-05-19 17:29:16.020815 D | ceph-spec: wjkim : objNew.Name %srook-ceph-detect-version-2lnnf
2024-05-19 17:29:16.020818 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:29:16.075660 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-5nhh7"
2024-05-19 17:29:16.075774 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:29:16.075790 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-5nhh7
2024-05-19 17:29:16.075798 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:29:22.993727 D | ceph-spec: update event from a CR: "builtin-mgr"
2024-05-19 17:29:22.993758 D | ceph-spec: update event on CephBlockPool CR
2024-05-19 17:29:22.993991 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-19 17:29:22.994356 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:29:22.994501 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:29:22.994713 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:29:22.995678 D | ceph-block-pool-controller: pool "rook-ceph/builtin-mgr" status updated to "Progressing"
2024-05-19 17:29:22.995887 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-19 17:29:22.995927 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-19 17:29:22.996252 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-19 17:29:23.765105 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-19 17:29:23.766000 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-19 17:29:23.766149 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:29:23.766260 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:29:23.766463 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:29:24.337875 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-19 17:29:24.338249 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-19 17:29:24.338383 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:29:24.338759 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:29:24.338835 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:29:24.625276 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-19 17:29:28.790983 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-19 17:29:28.791001 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-19 17:29:32.998209 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-19 17:29:32.998242 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-19 17:29:32.998249 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-19 17:29:33.768598 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-19 17:29:33.769604 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:29:33.769948 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:29:33.770195 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:29:34.317520 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-19 17:29:34.339624 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-19 17:29:34.340243 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:29:34.340484 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:29:34.341027 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:29:34.496832 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-19 17:29:43.023035 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-19 17:29:43.023406 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-19 17:29:43.023437 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-19 17:29:44.318936 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-19 17:29:44.319391 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:29:44.319599 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:29:44.319763 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:29:44.907950 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-19 17:29:44.908348 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-19 17:29:44.908689 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:29:44.908878 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:29:44.908973 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:29:45.032156 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-19 17:29:53.055327 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-19 17:29:53.055396 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-19 17:29:53.055421 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-19 17:29:54.914382 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-19 17:29:54.918928 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:29:54.939648 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:29:54.940610 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:29:55.424095 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-19 17:29:55.424972 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-19 17:29:55.425434 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:29:55.425737 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:29:55.425984 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:29:55.739122 D | clusterdisruption-controller: ceph "rook-ceph" cluster not ready, cannot check status yet.
2024-05-19 17:30:01.171612 D | CmdReporter: job rook-ceph-detect-version has returned results
2024-05-19 17:30:01.180565 D | ceph-spec: update event from a CR: "rook-ceph-detect-version-2lnnf"
2024-05-19 17:30:01.184792 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-detect-version-2lnnf
2024-05-19 17:30:01.185428 D | ceph-spec: wjkim : objNew.Name %srook-ceph-detect-version-2lnnf
2024-05-19 17:30:01.185434 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:01.297635 I | ceph-spec: detected ceph image version: "18.2.2-0 reef"
2024-05-19 17:30:01.297696 I | ceph-cluster-controller: validating ceph version from provided image
2024-05-19 17:30:01.300417 D | ceph-cluster-controller: cluster not initialized, nothing to validate. clusterInfo is nil
2024-05-19 17:30:01.300456 I | ceph-cluster-controller: cluster "rook-ceph": version "18.2.2-0 reef" detected for image "quay.io/ceph/ceph:v18"
2024-05-19 17:30:01.378881 D | ceph-spec: update event from a CR: "rook-ceph-detect-version-2lnnf"
2024-05-19 17:30:01.378936 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-detect-version-2lnnf
2024-05-19 17:30:01.379112 D | ceph-spec: wjkim : objNew.Name %srook-ceph-detect-version-2lnnf
2024-05-19 17:30:01.379118 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:01.391732 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring the Ceph cluster"
2024-05-19 17:30:01.392060 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-19 17:30:01.392116 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-19 17:30:01.400188 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-19 17:30:01.400210 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-19 17:30:01.400229 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-19 17:30:01.400232 D | ceph-spec: object "rook-ceph-detect-version" did not match on delete
2024-05-19 17:30:01.401179 D | ceph-spec: update event from a CR: "rook-ceph-detect-version-2lnnf"
2024-05-19 17:30:01.401213 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-detect-version-2lnnf
2024-05-19 17:30:01.401218 D | ceph-spec: wjkim : objNew.Name %srook-ceph-detect-version-2lnnf
2024-05-19 17:30:01.401220 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:01.402620 E | ceph-spec: failed to update cluster condition to {Type:Progressing Status:True Reason:ClusterProgressing Message:Configuring the Ceph cluster LastHeartbeatTime:2024-05-19 17:30:01.391726188 +0000 UTC m=+49.462405049 LastTransitionTime:2024-05-19 17:30:01.391726103 +0000 UTC m=+49.462404992}. failed to update object "rook-ceph/my-cluster" status: Operation cannot be fulfilled on cephclusters.ceph.rook.io "my-cluster": the object has been modified; please apply your changes to the latest version and try again
2024-05-19 17:30:01.404609 D | ceph-cluster-controller: cluster helm chart is not configured, not adding helm annotations to configmap
2024-05-19 17:30:01.413669 I | ceph-cluster-controller: created placeholder configmap for ceph overrides "rook-config-override"
2024-05-19 17:30:01.413680 D | ceph-cluster-controller: monitors are about to reconcile, executing pre actions
2024-05-19 17:30:01.413732 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mons"
2024-05-19 17:30:01.427170 D | op-mon: Acquiring lock for mon orchestration
2024-05-19 17:30:01.427254 D | op-mon: Acquired lock for mon orchestration
2024-05-19 17:30:01.427262 I | op-mon: start running mons
2024-05-19 17:30:01.427265 D | op-mon: establishing ceph cluster info
2024-05-19 17:30:01.427506 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-19 17:30:01.427593 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-19 17:30:01.434375 D | exec: Running command: ceph-authtool --create-keyring /var/lib/rook/rook-ceph/mon.keyring --gen-key -n mon. --cap mon 'allow *'
2024-05-19 17:30:01.470475 D | exec: Running command: ceph-authtool --create-keyring /var/lib/rook/rook-ceph/client.admin.keyring --gen-key -n client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mgr 'allow *' --cap mds 'allow'
2024-05-19 17:30:01.517890 I | ceph-spec: creating mon secrets for a new cluster
2024-05-19 17:30:01.583380 I | op-mon: existing maxMonID not found or failed to load. configmaps "rook-ceph-mon-endpoints" not found
2024-05-19 17:30:01.594505 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":[],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data: mapping:{"node":{}} maxMonId:-1 outOfQuorum:]
2024-05-19 17:30:01.787605 D | op-config: creating config secret "rook-ceph-config"
2024-05-19 17:30:02.004115 D | op-config: updating config secret "rook-ceph-config"
2024-05-19 17:30:02.295608 D | ceph-spec: update event from a CR: "rook-ceph-detect-version-2lnnf"
2024-05-19 17:30:02.296279 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-detect-version-2lnnf
2024-05-19 17:30:02.296331 D | ceph-spec: wjkim : objNew.Name %srook-ceph-detect-version-2lnnf
2024-05-19 17:30:02.296334 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:02.384862 D | cephclient: No ceph configuration override to merge as "rook-config-override" configmap is empty
2024-05-19 17:30:02.384939 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2024-05-19 17:30:02.385507 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2024-05-19 17:30:02.385630 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-19 17:30:02.991836 D | op-cfg-keyring: creating secret for rook-ceph-mons-keyring
2024-05-19 17:30:03.075251 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-19 17:30:03.075560 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-19 17:30:03.075803 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-19 17:30:03.382241 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-19 17:30:03.581579 D | op-cfg-keyring: creating secret for rook-ceph-admin-keyring
2024-05-19 17:30:03.786705 I | ceph-spec: parsing mon endpoints: 
2024-05-19 17:30:03.786749 W | ceph-spec: ignoring invalid monitor 
2024-05-19 17:30:03.786781 D | ceph-spec: loaded: maxMonID=-1, mons=map[], assignment=&{Schedule:map[]}
2024-05-19 17:30:03.786908 I | op-k8sutil: ROOK_OBC_WATCH_OPERATOR_NAMESPACE="true" (configmap)
2024-05-19 17:30:03.787597 I | op-k8sutil: ROOK_OBC_PROVISIONER_NAME_PREFIX="" (default)
2024-05-19 17:30:03.787608 I | op-bucket-prov: ceph bucket provisioner launched watching for provisioner "rook-ceph.ceph.rook.io/bucket"
2024-05-19 17:30:03.788567 I | op-bucket-prov: successfully reconciled bucket provisioner
I0519 17:30:03.788877       1 manager.go:135] "msg"="starting provisioner" "logger"="objectbucket.io/provisioner-manager" "name"="rook-ceph.ceph.rook.io/bucket"
2024-05-19 17:30:03.816128 D | ceph-spec: update event from a CR: "rook-ceph-detect-version-2lnnf"
2024-05-19 17:30:03.816254 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-detect-version-2lnnf
2024-05-19 17:30:03.816266 D | ceph-spec: wjkim : objNew.Name %srook-ceph-detect-version-2lnnf
2024-05-19 17:30:03.816293 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:04.438922 I | op-mon: targeting the mon count 1
2024-05-19 17:30:04.458813 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-a DaemonName:a PublicIP: Port:6789 Zone: NodeName: DataPathMap:0xc0012e7ec0 UseHostNetwork:false}
2024-05-19 17:30:04.616326 D | ceph-spec: update event from a CR: "rook-ceph-detect-version-2lnnf"
2024-05-19 17:30:04.616384 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-detect-version-2lnnf
2024-05-19 17:30:04.616387 D | ceph-spec: wjkim : objNew.Name %srook-ceph-detect-version-2lnnf
2024-05-19 17:30:04.616388 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:04.621417 I | op-mon: created canary deployment rook-ceph-mon-a-canary
2024-05-19 17:30:04.658307 I | op-mon: waiting for canary pod creation rook-ceph-mon-a-canary
2024-05-19 17:30:04.839798 D | ceph-spec: update event from a CR: "rook-ceph-detect-version-2lnnf"
2024-05-19 17:30:04.839826 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-detect-version-2lnnf
2024-05-19 17:30:04.839829 D | ceph-spec: wjkim : objNew.Name %srook-ceph-detect-version-2lnnf
2024-05-19 17:30:04.839831 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:04.840165 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-19 17:30:04.840174 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:04.885783 D | ceph-spec: create event from a CR: "rook-ceph-mon-a-canary-58bccfff95-98stx"
2024-05-19 17:30:04.885893 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:04.886003 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:30:04.886038 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:30:04.898299 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-mon-a-canary-58bccfff95-98stx"
2024-05-19 17:30:04.905495 D | ceph-spec: delete event from a CR: "rook-ceph-detect-version-2lnnf"
2024-05-19 17:30:04.905560 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-detect-version-2lnnf
2024-05-19 17:30:04.905573 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-detect-version-2lnnf" not found
2024-05-19 17:30:04.918636 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-detect-version-2lnnf
2024-05-19 17:30:04.921763 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-detect-version-2lnnf" not found
2024-05-19 17:30:04.934823 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-detect-version-2lnnf
2024-05-19 17:30:04.935102 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-detect-version-2lnnf" not found
2024-05-19 17:30:04.946287 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-19 17:30:04.946335 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:04.946772 D | ceph-spec: update event from a CR: "rook-ceph-mon-a-canary-58bccfff95-98stx"
2024-05-19 17:30:04.946788 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:04.946790 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:04.946792 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:04.956685 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-detect-version-2lnnf
2024-05-19 17:30:04.956753 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-detect-version-2lnnf" not found
2024-05-19 17:30:04.981548 D | ceph-spec: update event from a CR: "rook-ceph-mon-a-canary-58bccfff95-98stx"
2024-05-19 17:30:04.981624 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:04.981629 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:04.981631 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:04.990731 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-19 17:30:04.990775 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:05.002868 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-detect-version-2lnnf
2024-05-19 17:30:05.003256 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-detect-version-2lnnf" not found
2024-05-19 17:30:05.085612 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-detect-version-2lnnf
2024-05-19 17:30:05.085868 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-detect-version-2lnnf" not found
2024-05-19 17:30:05.248232 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-detect-version-2lnnf
2024-05-19 17:30:05.248527 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-detect-version-2lnnf" not found
2024-05-19 17:30:05.484432 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-19 17:30:05.484769 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:30:05.484928 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:30:05.485007 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:05.605643 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-detect-version-2lnnf
2024-05-19 17:30:05.607258 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-detect-version-2lnnf" not found
2024-05-19 17:30:06.271958 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-detect-version-2lnnf
2024-05-19 17:30:06.272288 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-detect-version-2lnnf" not found
2024-05-19 17:30:06.498152 D | clusterdisruption-controller: ceph "rook-ceph" cluster failed to check cluster health. failed to get status. . unable to get monitor info from DNS SRV with service name: ceph-mon
2024-05-19T17:30:06.483+0000 7f4aa6f34700 -1 failed for service _ceph-mon._tcp
[errno 2] RADOS object not found (error connecting to the cluster): exit status 1
2024-05-19 17:30:06.498356 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-19 17:30:06.499604 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:30:06.499716 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:30:06.499766 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:07.294717 D | clusterdisruption-controller: ceph "rook-ceph" cluster failed to check cluster health. failed to get status. . unable to get monitor info from DNS SRV with service name: ceph-mon
[errno 2] RADOS object not found (error connecting to the cluster): exit status 1
2024-05-19 17:30:07.554064 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-detect-version-2lnnf
2024-05-19 17:30:07.554339 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-detect-version-2lnnf" not found
2024-05-19 17:30:09.705500 I | op-mon: canary monitor deployment rook-ceph-mon-a-canary scheduled to minikube
2024-05-19 17:30:09.705521 I | op-mon: mon a assigned to node minikube
2024-05-19 17:30:09.705524 D | op-mon: using internal IP 192.168.49.2 for node minikube
2024-05-19 17:30:09.705544 D | op-mon: mons have been scheduled
2024-05-19 17:30:09.788974 I | op-mon: cleaning up canary monitor deployment "rook-ceph-mon-a-canary"
2024-05-19 17:30:10.034038 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-19 17:30:10.034201 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:10.040196 I | op-mon: creating mon a
2024-05-19 17:30:10.040592 D | op-k8sutil: creating service rook-ceph-mon-a
2024-05-19 17:30:10.158784 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-detect-version-2lnnf
2024-05-19 17:30:10.158970 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-detect-version-2lnnf" not found
2024-05-19 17:30:10.315438 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-19 17:30:10.315472 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:10.377227 D | op-k8sutil: created service rook-ceph-mon-a
2024-05-19 17:30:10.379239 I | op-mon: mon "a" cluster IP is 10.104.237.69
2024-05-19 17:30:10.602170 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2024-05-19 17:30:10.706558 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.104.237.69:6789"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.104.237.69:6789 mapping:{"node":{"a":{"Name":"minikube","Hostname":"minikube","Address":"192.168.49.2"}}} maxMonId:-1 outOfQuorum:]
2024-05-19 17:30:10.706791 D | op-mon: mons were added or removed from the endpoints cm
2024-05-19 17:30:10.706802 I | op-mon: monitor endpoints changed, updating the bootstrap peer token
2024-05-19 17:30:10.707938 D | ceph-spec: object "rook-ceph-mon-endpoints" matched on update
2024-05-19 17:30:10.707947 D | ceph-spec: do not reconcile on configmap "rook-ceph-mon-endpoints"
2024-05-19 17:30:10.707950 D | op-mon: mons were added or removed from the endpoints cm
2024-05-19 17:30:10.707951 I | op-mon: monitor endpoints changed, updating the bootstrap peer token
2024-05-19 17:30:10.759867 D | op-config: updating config secret "rook-ceph-config"
2024-05-19 17:30:10.760487 D | ceph-spec: update event from a CR: "rook-ceph-mon-a-canary-58bccfff95-98stx"
2024-05-19 17:30:10.760574 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:10.760580 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:10.760584 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:10.762133 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-19 17:30:10.762199 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-19 17:30:10.762208 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-19 17:30:10.888536 D | ceph-spec: object "rook-ceph-config" matched on update
2024-05-19 17:30:10.888734 D | ceph-spec: do not reconcile on "rook-ceph-config" secret changes
2024-05-19 17:30:10.955804 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-19 17:30:10.955828 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:10.978660 D | cephclient: No ceph configuration override to merge as "rook-config-override" configmap is empty
2024-05-19 17:30:10.978704 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2024-05-19 17:30:10.978949 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2024-05-19 17:30:10.978978 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-19 17:30:11.006548 I | op-mon: 0 of 1 expected mons are ready. creating or updating deployments without checking quorum in attempt to achieve a healthy mon cluster
2024-05-19 17:30:11.006665 D | op-mon: monConfig: &{ResourceName:rook-ceph-mon-a DaemonName:a PublicIP:10.104.237.69 Port:6789 Zone: NodeName: DataPathMap:0xc0012e7ec0 UseHostNetwork:false}
2024-05-19 17:30:11.133701 D | op-mon: adding host path volume source to mon deployment rook-ceph-mon-a
2024-05-19 17:30:11.133731 D | op-mon: Starting mon: rook-ceph-mon-a
2024-05-19 17:30:11.234387 I | op-mon: updating maxMonID from -1 to 0
2024-05-19 17:30:11.325919 D | ceph-spec: create event from a CR: "rook-ceph-mon-a-6cf686549b-2snf4"
2024-05-19 17:30:11.326148 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-mon-a-6cf686549b-2snf4
2024-05-19 17:30:11.326294 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:30:11.326310 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:30:11.326322 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-mon-a-6cf686549b-2snf4"
2024-05-19 17:30:11.327021 D | ceph-spec: object "rook-ceph-mon-endpoints" matched on update
2024-05-19 17:30:11.327040 D | ceph-spec: do not reconcile on configmap "rook-ceph-mon-endpoints"
2024-05-19 17:30:11.338261 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2024-05-19 17:30:11.338280 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:11.555496 D | ceph-nodedaemon-controller: "rook-ceph-mon-a-6cf686549b-2snf4" is a ceph pod!
2024-05-19 17:30:11.555742 D | ceph-nodedaemon-controller: reconciling node: "minikube"
2024-05-19 17:30:11.556024 D | ceph-nodedaemon-controller: secret "rook-ceph-crash-collector-keyring" in namespace "rook-ceph" not found. retrying in "30s". Secret "rook-ceph-crash-collector-keyring" not found
2024-05-19 17:30:11.556097 D | ceph-spec: update event from a CR: "rook-ceph-mon-a-6cf686549b-2snf4"
2024-05-19 17:30:11.556112 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mon-a-6cf686549b-2snf4
2024-05-19 17:30:11.556123 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mon-a-6cf686549b-2snf4
2024-05-19 17:30:11.556132 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:11.638021 D | op-mon: updating config map rook-ceph-mon-endpoints that already exists
2024-05-19 17:30:11.709960 D | ceph-spec: update event from a CR: "rook-ceph-mon-a-6cf686549b-2snf4"
2024-05-19 17:30:11.709983 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mon-a-6cf686549b-2snf4
2024-05-19 17:30:11.709988 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mon-a-6cf686549b-2snf4
2024-05-19 17:30:11.709990 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:11.791347 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2024-05-19 17:30:11.797470 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:11.806034 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.104.237.69:6789"],"cephFS":{"netNamespaceFilePath":"","subvolumeGroup":"","kernelMountOptions":"","fuseMountOptions":""},"rbd":{"netNamespaceFilePath":"","radosNamespace":""},"nfs":{"netNamespaceFilePath":""},"readAffinity":{"enabled":false,"crushLocationLabels":null},"namespace":""}] data:a=10.104.237.69:6789 mapping:{"node":{"a":{"Name":"minikube","Hostname":"minikube","Address":"192.168.49.2"}}} maxMonId:0 outOfQuorum:]
2024-05-19 17:30:11.806157 I | op-mon: waiting for mon quorum with [a]
2024-05-19 17:30:11.825346 I | op-mon: mon a is not yet running
2024-05-19 17:30:11.825390 I | op-mon: mons running: []
2024-05-19 17:30:11.825402 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:12.051212 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2024-05-19 17:30:12.051231 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:12.390160 D | operator: number of goroutines 488
2024-05-19 17:30:12.658870 D | ceph-spec: update event from a CR: "rook-ceph-mon-a-canary-58bccfff95-98stx"
2024-05-19 17:30:12.658904 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:12.658908 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:12.658910 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:13.080170 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-19 17:30:13.080267 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-19 17:30:13.080278 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-19 17:30:15.285612 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-detect-version-2lnnf
2024-05-19 17:30:15.288467 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-detect-version-2lnnf" not found
2024-05-19 17:30:16.441661 D | ceph-spec: update event from a CR: "rook-ceph-mon-a-6cf686549b-2snf4"
2024-05-19 17:30:16.441840 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mon-a-6cf686549b-2snf4
2024-05-19 17:30:16.441846 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mon-a-6cf686549b-2snf4
2024-05-19 17:30:16.441848 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:16.499059 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-19 17:30:16.499509 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:30:16.499653 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:30:16.499751 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:17.587081 D | ceph-spec: update event from a CR: "rook-ceph-mon-a-6cf686549b-2snf4"
2024-05-19 17:30:17.587161 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mon-a-6cf686549b-2snf4
2024-05-19 17:30:17.587173 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mon-a-6cf686549b-2snf4
2024-05-19 17:30:17.587174 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:18.523779 D | ceph-spec: update event from a CR: "rook-ceph-mon-a-6cf686549b-2snf4"
2024-05-19 17:30:18.523809 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mon-a-6cf686549b-2snf4
2024-05-19 17:30:18.523814 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mon-a-6cf686549b-2snf4
2024-05-19 17:30:18.523816 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:19.598482 D | ceph-spec: update event from a CR: "rook-ceph-mon-a-6cf686549b-2snf4"
2024-05-19 17:30:19.598510 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mon-a-6cf686549b-2snf4
2024-05-19 17:30:19.598513 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mon-a-6cf686549b-2snf4
2024-05-19 17:30:19.598514 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:23.082055 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-19 17:30:23.082158 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-19 17:30:23.082176 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-19 17:30:25.530591 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-detect-version-2lnnf
2024-05-19 17:30:25.530815 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-detect-version-2lnnf" not found
2024-05-19 17:30:27.616902 D | op-mon: failed to get quorum_status. mon quorum status failed: exit status 1
2024-05-19 17:30:30.231016 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-19 17:30:30.231036 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-19 17:30:32.009570 D | clusterdisruption-controller: ceph "rook-ceph" cluster failed to check cluster health. failed to get status. . timed out: exit status 1
2024-05-19 17:30:32.009823 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-19 17:30:32.009987 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:30:32.010072 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:30:32.010151 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:32.636601 I | op-mon: mons running: [a]
2024-05-19 17:30:32.636627 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:32.673865 D | ceph-spec: update event from a CR: "rook-ceph-mon-a-6cf686549b-2snf4"
2024-05-19 17:30:32.673902 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mon-a-6cf686549b-2snf4
2024-05-19 17:30:32.673905 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mon-a-6cf686549b-2snf4
2024-05-19 17:30:32.673907 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:32.780698 D | ceph-spec: object "rook-ceph-mon-a" matched on update
2024-05-19 17:30:32.780716 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:33.055723 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: []. pg health: "cluster has no PGs"
2024-05-19 17:30:33.055797 I | clusterdisruption-controller: all PGs are active+clean. Restoring default OSD pdb settings
2024-05-19 17:30:33.055805 I | clusterdisruption-controller: creating the default pdb "rook-ceph-osd" with maxUnavailable=1 for all osd
2024-05-19 17:30:33.082595 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-19 17:30:33.082615 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-19 17:30:33.082622 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-19 17:30:33.083541 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2024-05-19 17:30:33.095482 I | op-mon: Monitors in quorum: [a]
2024-05-19 17:30:33.095674 I | op-mon: mons created: 1
2024-05-19 17:30:33.095818 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:33.113883 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-19 17:30:33.114879 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:30:33.114995 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:30:33.115103 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:33.548326 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: []. pg health: "cluster has no PGs"
2024-05-19 17:30:33.556688 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1}}
2024-05-19 17:30:33.557108 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1}}
2024-05-19 17:30:33.557914 I | op-mon: waiting for mon quorum with [a]
2024-05-19 17:30:33.593730 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2024-05-19 17:30:33.605958 I | op-mon: mons running: [a]
2024-05-19 17:30:33.606026 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:33.940804 I | op-mon: Monitors in quorum: [a]
2024-05-19 17:30:33.941096 I | op-config: applying ceph settings:
[global]
mon cluster log file    = 
mon allow pool size one = true
mon allow pool delete   = true
2024-05-19 17:30:33.941190 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/1238774011 -o /var/lib/rook/1238774011.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:34.862360 I | op-config: successfully applied settings to the mon configuration database
2024-05-19 17:30:34.863527 I | op-config: applying ceph settings:
[global]
log to file = false
2024-05-19 17:30:34.863645 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/2782161683 -o /var/lib/rook/2782161683.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:36.097652 I | op-config: successfully applied settings to the mon configuration database
2024-05-19 17:30:36.098478 I | op-config: deleting "global" "log file" option from the mon configuration database
2024-05-19 17:30:36.098591 D | exec: Running command: ceph config rm global log_file --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:36.589916 I | op-config: successfully deleted "log file" option from the mon configuration database
2024-05-19 17:30:36.589942 I | ceph-spec: not applying network settings for cluster "rook-ceph" ceph networks
2024-05-19 17:30:36.589965 D | op-mon: mon endpoints used are: a=10.104.237.69:6789
2024-05-19 17:30:36.589967 D | op-mon: managePodBudgets is set, but mon-count <= 2. Not creating a disruptionbudget for Mons
2024-05-19 17:30:36.590148 D | op-mon: skipping check for orphaned mon pvcs since using the host path
2024-05-19 17:30:36.590153 D | op-mon: Released lock for mon orchestration
2024-05-19 17:30:36.590168 D | ceph-cluster-controller: monitors are up and running, executing post actions
2024-05-19 17:30:36.590382 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2024-05-19 17:30:36.590477 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-provisioner mon profile rbd, allow command 'osd blocklist' mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:37.178270 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2024-05-19 17:30:37.178355 D | exec: Running command: ceph auth get-or-create-key client.csi-rbd-node mon profile rbd mgr allow rw osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:37.651609 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2024-05-19 17:30:37.653113 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-provisioner mon allow r, allow command 'osd blocklist' mgr allow rw osd allow rw tag cephfs metadata=* mds allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:38.435892 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2024-05-19 17:30:38.435972 D | exec: Running command: ceph auth get-or-create-key client.csi-cephfs-node mon allow r mgr allow rw osd allow rw tag cephfs *=* mds allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:38.926728 D | op-cfg-keyring: creating secret for rook-csi-cephfs-provisioner
2024-05-19 17:30:38.937586 D | op-cfg-keyring: creating secret for rook-csi-cephfs-node
2024-05-19 17:30:38.945905 D | op-cfg-keyring: creating secret for rook-csi-rbd-provisioner
2024-05-19 17:30:38.957599 D | op-cfg-keyring: creating secret for rook-csi-rbd-node
2024-05-19 17:30:38.977141 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2024-05-19 17:30:38.977178 I | cephclient: getting or creating ceph auth key "client.crash"
2024-05-19 17:30:38.977239 D | exec: Running command: ceph auth get-or-create-key client.crash mon allow profile crash mgr allow rw --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:39.621435 D | op-cfg-keyring: creating secret for rook-ceph-crash-collector-keyring
2024-05-19 17:30:39.662260 I | ceph-nodedaemon-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2024-05-19 17:30:39.662754 I | cephclient: getting or creating ceph auth key "client.ceph-exporter"
2024-05-19 17:30:39.662962 D | exec: Running command: ceph auth get-or-create-key client.ceph-exporter mon allow profile ceph-exporter mgr allow r osd allow r mds allow r --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:40.340864 D | op-cfg-keyring: creating secret for rook-ceph-exporter-keyring
2024-05-19 17:30:40.353924 I | ceph-nodedaemon-controller: created kubernetes exporter secret for cluster "rook-ceph"
2024-05-19 17:30:40.353999 I | op-config: deleting "global" "ms_cluster_mode" option from the mon configuration database
2024-05-19 17:30:40.354078 D | exec: Running command: ceph config rm global ms_cluster_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:40.695832 I | op-config: successfully deleted "ms_cluster_mode" option from the mon configuration database
2024-05-19 17:30:40.695863 I | op-config: deleting "global" "ms_service_mode" option from the mon configuration database
2024-05-19 17:30:40.695883 D | exec: Running command: ceph config rm global ms_service_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:41.047646 I | op-config: successfully deleted "ms_service_mode" option from the mon configuration database
2024-05-19 17:30:41.047794 I | op-config: deleting "global" "ms_client_mode" option from the mon configuration database
2024-05-19 17:30:41.047934 D | exec: Running command: ceph config rm global ms_client_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:41.437478 I | op-config: successfully deleted "ms_client_mode" option from the mon configuration database
2024-05-19 17:30:41.437673 I | op-config: deleting "global" "rbd_default_map_options" option from the mon configuration database
2024-05-19 17:30:41.437833 D | exec: Running command: ceph config rm global rbd_default_map_options --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:41.558797 D | ceph-nodedaemon-controller: reconciling node: "minikube"
2024-05-19 17:30:41.560287 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-19 17:30:41.591067 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube". operation: "created"
2024-05-19 17:30:41.591098 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-19 17:30:41.680663 D | op-k8sutil: created service rook-ceph-exporter
2024-05-19 17:30:41.680699 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-19 17:30:41.813560 D | ceph-spec: create event from a CR: "rook-ceph-exporter-minikube-5567db46db-8fmrg"
2024-05-19 17:30:41.813964 D | ceph-spec: object "rook-ceph-exporter-minikube" matched on update
2024-05-19 17:30:41.813989 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:41.814724 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:41.814833 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:30:41.814843 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:30:41.814851 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-exporter-minikube-5567db46db-8fmrg"
2024-05-19 17:30:41.843851 D | ceph-nodedaemon-controller: "rook-ceph-exporter-minikube-5567db46db-8fmrg" is a ceph pod!
2024-05-19 17:30:41.843939 D | ceph-nodedaemon-controller: reconciling node: "minikube"
2024-05-19 17:30:41.844773 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-19 17:30:41.848218 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-5567db46db-8fmrg"
2024-05-19 17:30:41.848545 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:41.848573 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:41.848595 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:41.874846 D | ceph-spec: object "rook-ceph-exporter-minikube" matched on update
2024-05-19 17:30:41.874921 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:41.933337 D | ceph-nodedaemon-controller: ceph exporter unchanged on node "minikube"
2024-05-19 17:30:41.939091 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-19 17:30:41.939778 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-5567db46db-8fmrg"
2024-05-19 17:30:41.939846 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:41.939849 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:41.939850 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:42.012117 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-19 17:30:42.012458 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:30:42.012620 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:30:42.012711 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:42.049932 D | ceph-spec: object "rook-ceph-exporter-minikube" matched on update
2024-05-19 17:30:42.049973 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:42.992402 I | op-config: successfully deleted "rbd_default_map_options" option from the mon configuration database
2024-05-19 17:30:42.992598 I | op-config: deleting "global" "ms_osd_compress_mode" option from the mon configuration database
2024-05-19 17:30:42.992872 D | exec: Running command: ceph config rm global ms_osd_compress_mode --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:43.083732 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-19 17:30:43.083806 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-19 17:30:43.083815 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-19 17:30:43.232275 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: []. pg health: "cluster has no PGs"
2024-05-19 17:30:43.235862 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2024-05-19 17:30:43.387636 I | op-config: successfully deleted "ms_osd_compress_mode" option from the mon configuration database
2024-05-19 17:30:43.388124 I | op-config: applying ceph settings:
[global]
osd_pool_default_size          = 1
bdev_flock_retry               = 20
bluefs_buffered_io             = false
mon_data_avail_warn            = 10
mon_warn_on_pool_no_redundancy = false
2024-05-19 17:30:43.388161 D | exec: Running command: ceph config assimilate-conf -i /var/lib/rook/3337595954 -o /var/lib/rook/3337595954.out --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:43.658778 D | ceph-spec: update event from a CR: "rook-ceph-mon-a-canary-58bccfff95-98stx"
2024-05-19 17:30:43.658807 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:43.658810 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:43.658812 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:44.159523 I | op-config: successfully applied settings to the mon configuration database
2024-05-19 17:30:44.164732 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2024-05-19 17:30:44.165566 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2024-05-19 17:30:44.166110 D | exec: Running command: ceph auth get-or-create-key client.rbd-mirror-peer mon profile rbd-mirror-peer osd profile rbd --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:44.853384 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "my-cluster"
2024-05-19 17:30:44.853529 D | ceph-spec: store cluster-rbd-mirror bootstrap token in a Kubernetes Secret "cluster-peer-token-my-cluster" in namespace "rook-ceph"
2024-05-19 17:30:44.853559 D | op-k8sutil: creating secret cluster-peer-token-my-cluster
2024-05-19 17:30:44.856242 D | ceph-spec: update event from a CR: "rook-ceph-mon-a-canary-58bccfff95-98stx"
2024-05-19 17:30:44.856263 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:44.856265 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:44.856267 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:44.905818 D | op-k8sutil: created secret cluster-peer-token-my-cluster
2024-05-19 17:30:44.906017 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph Mgr(s)"
2024-05-19 17:30:44.920764 D | ceph-spec: update event from a CR: "rook-ceph-mon-a-canary-58bccfff95-98stx"
2024-05-19 17:30:44.920829 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:44.920842 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:44.920843 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:44.986863 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-19 17:30:44.986933 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-19 17:30:44.987077 D | ceph-spec: delete event from a CR: "rook-ceph-mon-a-canary-58bccfff95-98stx"
2024-05-19 17:30:44.987352 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:44.987446 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-mon-a-canary-58bccfff95-98stx" not found
2024-05-19 17:30:44.988061 I | op-mgr: start running mgr
2024-05-19 17:30:44.992724 I | cephclient: getting or creating ceph auth key "mgr.a"
2024-05-19 17:30:44.992791 D | exec: Running command: ceph auth get-or-create-key mgr.a mon allow profile mgr mds allow * osd allow * --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:44.993384 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:44.993480 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-mon-a-canary-58bccfff95-98stx" not found
2024-05-19 17:30:45.010084 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:45.011308 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-mon-a-canary-58bccfff95-98stx" not found
2024-05-19 17:30:45.032199 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:45.032251 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-mon-a-canary-58bccfff95-98stx" not found
2024-05-19 17:30:45.039966 D | ceph-spec: object "rook-ceph-mon-a-canary" matched on update
2024-05-19 17:30:45.040053 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:45.073513 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:45.073774 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-mon-a-canary-58bccfff95-98stx" not found
2024-05-19 17:30:45.119849 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-05-19 17:30:45.119872 D | ceph-spec: do not reconcile "rook-ceph-mon-a-canary" on monitor canary deployments
2024-05-19 17:30:45.119878 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-05-19 17:30:45.119881 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-05-19 17:30:45.119905 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-05-19 17:30:45.119908 D | ceph-spec: object "rook-ceph-mon-a-canary" did not match on delete
2024-05-19 17:30:45.155047 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:45.155203 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-mon-a-canary-58bccfff95-98stx" not found
2024-05-19 17:30:45.315803 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:45.315860 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-mon-a-canary-58bccfff95-98stx" not found
2024-05-19 17:30:45.402374 D | op-mgr: legacy mgr key "rook-ceph-mgr-a" is already removed
2024-05-19 17:30:45.403781 D | op-cfg-keyring: creating secret for rook-ceph-mgr-a-keyring
2024-05-19 17:30:45.411418 D | op-mgr: mgrConfig: &{ResourceName:rook-ceph-mgr-a DaemonID:a DataPathMap:0xc000ef12f0}
2024-05-19 17:30:45.439366 I | op-config: setting "mon"="auth_allow_insecure_global_id_reclaim"="false" option to the mon configuration database
2024-05-19 17:30:45.439409 D | exec: Running command: ceph config set mon auth_allow_insecure_global_id_reclaim false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:30:45.537558 D | ceph-spec: create event from a CR: "rook-ceph-mgr-a-5b697dcffd-7flbh"
2024-05-19 17:30:45.537601 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2024-05-19 17:30:45.537610 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:45.537681 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-mgr-a-5b697dcffd-7flbh
2024-05-19 17:30:45.537733 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:30:45.537741 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:30:45.537757 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-mgr-a-5b697dcffd-7flbh"
2024-05-19 17:30:45.580849 D | ceph-nodedaemon-controller: "rook-ceph-mgr-a-5b697dcffd-7flbh" is a ceph pod!
2024-05-19 17:30:45.580988 D | ceph-nodedaemon-controller: reconciling node: "minikube"
2024-05-19 17:30:45.581467 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-19 17:30:45.582391 D | ceph-spec: update event from a CR: "rook-ceph-mgr-a-5b697dcffd-7flbh"
2024-05-19 17:30:45.582435 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mgr-a-5b697dcffd-7flbh
2024-05-19 17:30:45.582440 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mgr-a-5b697dcffd-7flbh
2024-05-19 17:30:45.582442 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:45.639128 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:45.639203 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-mon-a-canary-58bccfff95-98stx" not found
2024-05-19 17:30:45.654858 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2024-05-19 17:30:45.654879 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:45.706119 D | ceph-spec: update event from a CR: "rook-ceph-mgr-a-5b697dcffd-7flbh"
2024-05-19 17:30:45.706146 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mgr-a-5b697dcffd-7flbh
2024-05-19 17:30:45.706149 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mgr-a-5b697dcffd-7flbh
2024-05-19 17:30:45.706150 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:45.713898 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube". operation: "updated"
2024-05-19 17:30:45.713927 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-19 17:30:45.714099 D | ceph-spec: object "rook-ceph-exporter-minikube" matched on update
2024-05-19 17:30:45.714111 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:45.771552 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2024-05-19 17:30:45.771580 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:45.813735 D | ceph-spec: object "rook-ceph-exporter-minikube" matched on update
2024-05-19 17:30:45.813758 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:45.825234 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-5567db46db-8fmrg"
2024-05-19 17:30:45.825268 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:45.825273 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:45.825275 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:45.833499 D | ceph-spec: object "rook-ceph-exporter-minikube" matched on update
2024-05-19 17:30:45.833517 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:45.858655 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-19 17:30:45.912955 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-5567db46db-8fmrg"
2024-05-19 17:30:45.913049 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:45.913059 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:45.913066 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:45.916620 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-19 17:30:46.033200 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-detect-version-2lnnf
2024-05-19 17:30:46.038255 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-detect-version-2lnnf" not found
2024-05-19 17:30:46.137703 D | ceph-spec: object "rook-ceph-exporter-minikube" matched on update
2024-05-19 17:30:46.137754 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:46.310316 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:46.314289 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-mon-a-canary-58bccfff95-98stx" not found
2024-05-19 17:30:46.428522 I | op-config: successfully set "mon"="auth_allow_insecure_global_id_reclaim"="false" option to the mon configuration database
2024-05-19 17:30:46.428739 I | op-config: insecure global ID is now disabled
2024-05-19 17:30:46.461271 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-19 17:30:45 +0000 UTC LastTransitionTime:2024-05-19 17:30:45 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-19 17:30:45 +0000 UTC LastTransitionTime:2024-05-19 17:30:45 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5b697dcffd" is progressing.}] CollisionCount:<nil>}
2024-05-19 17:30:47.061009 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-5567db46db-8fmrg"
2024-05-19 17:30:47.061037 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:47.061040 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:47.061042 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:47.599983 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:47.600317 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-mon-a-canary-58bccfff95-98stx" not found
2024-05-19 17:30:49.315063 D | ceph-spec: update event from a CR: "rook-ceph-mgr-a-5b697dcffd-7flbh"
2024-05-19 17:30:49.315093 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mgr-a-5b697dcffd-7flbh
2024-05-19 17:30:49.315096 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mgr-a-5b697dcffd-7flbh
2024-05-19 17:30:49.315099 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:49.498259 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-19 17:30:45 +0000 UTC LastTransitionTime:2024-05-19 17:30:45 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-19 17:30:45 +0000 UTC LastTransitionTime:2024-05-19 17:30:45 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5b697dcffd" is progressing.}] CollisionCount:<nil>}
2024-05-19 17:30:50.136591 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-5567db46db-8fmrg"
2024-05-19 17:30:50.136606 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:50.136609 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:50.136610 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:50.175826 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:50.176463 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-mon-a-canary-58bccfff95-98stx" not found
2024-05-19 17:30:50.421129 D | ceph-spec: update event from a CR: "rook-ceph-mgr-a-5b697dcffd-7flbh"
2024-05-19 17:30:50.421608 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mgr-a-5b697dcffd-7flbh
2024-05-19 17:30:50.424349 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mgr-a-5b697dcffd-7flbh
2024-05-19 17:30:50.424365 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:50.538530 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-5567db46db-8fmrg"
2024-05-19 17:30:50.538694 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:50.538708 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:50.538710 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:50.576127 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-5567db46db-8fmrg"
2024-05-19 17:30:50.576142 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:50.576145 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:50.576146 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:50.603759 D | ceph-nodedaemon-controller: "rook-ceph-exporter-minikube-5567db46db-8fmrg" is a ceph pod!
2024-05-19 17:30:50.603787 D | ceph-spec: delete event from a CR: "rook-ceph-exporter-minikube-5567db46db-8fmrg"
2024-05-19 17:30:50.603877 D | ceph-nodedaemon-controller: reconciling node: "minikube"
2024-05-19 17:30:50.605079 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:50.605146 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-5567db46db-8fmrg" not found
2024-05-19 17:30:50.606456 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-19 17:30:50.615339 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:50.615412 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-5567db46db-8fmrg" not found
2024-05-19 17:30:50.615740 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube". operation: "updated"
2024-05-19 17:30:50.615806 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-19 17:30:50.627366 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:50.630159 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-5567db46db-8fmrg" not found
2024-05-19 17:30:50.651966 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:50.652040 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-5567db46db-8fmrg" not found
2024-05-19 17:30:50.707057 D | ceph-spec: object "rook-ceph-exporter-minikube" matched on update
2024-05-19 17:30:50.707144 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:50.707342 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:50.707592 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-5567db46db-8fmrg" not found
2024-05-19 17:30:50.708794 D | ceph-spec: create event from a CR: "rook-ceph-exporter-minikube-595c5f6b68-bksvk"
2024-05-19 17:30:50.709134 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:30:50.709258 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:30:50.709262 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:30:50.709277 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-exporter-minikube-595c5f6b68-bksvk"
2024-05-19 17:30:50.751232 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-19 17:30:50.771480 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-595c5f6b68-bksvk"
2024-05-19 17:30:50.771497 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:30:50.771500 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:30:50.771501 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:50.772694 D | ceph-nodedaemon-controller: "rook-ceph-exporter-minikube-595c5f6b68-bksvk" is a ceph pod!
2024-05-19 17:30:50.789537 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:50.789794 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-5567db46db-8fmrg" not found
2024-05-19 17:30:50.806353 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-19 17:30:50.806533 D | ceph-nodedaemon-controller: reconciling node: "minikube"
2024-05-19 17:30:50.806744 D | ceph-spec: object "rook-ceph-exporter-minikube" matched on update
2024-05-19 17:30:50.806763 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:50.806860 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-19 17:30:50.833801 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube". operation: "updated"
2024-05-19 17:30:50.833854 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-19 17:30:50.854096 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-595c5f6b68-bksvk"
2024-05-19 17:30:50.854154 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:30:50.854158 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:30:50.854159 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:50.893736 D | ceph-spec: object "rook-ceph-exporter-minikube" matched on update
2024-05-19 17:30:50.894233 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:50.959607 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:50.960001 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-5567db46db-8fmrg" not found
2024-05-19 17:30:50.995667 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-19 17:30:51.122439 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-19 17:30:51.282047 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:51.282522 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-5567db46db-8fmrg" not found
2024-05-19 17:30:51.923053 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:51.923168 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-5567db46db-8fmrg" not found
2024-05-19 17:30:52.529754 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-19 17:30:45 +0000 UTC LastTransitionTime:2024-05-19 17:30:45 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-19 17:30:45 +0000 UTC LastTransitionTime:2024-05-19 17:30:45 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5b697dcffd" is progressing.}] CollisionCount:<nil>}
2024-05-19 17:30:53.090787 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-19 17:30:53.090810 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-19 17:30:53.090817 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-19 17:30:53.238402 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:53.238539 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-5567db46db-8fmrg" not found
2024-05-19 17:30:53.272352 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-595c5f6b68-bksvk"
2024-05-19 17:30:53.272372 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:30:53.272374 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:30:53.272375 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:54.313833 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-595c5f6b68-bksvk"
2024-05-19 17:30:54.313934 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:30:54.313947 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:30:54.313954 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:30:54.375896 D | ceph-spec: object "rook-ceph-exporter-minikube" matched on update
2024-05-19 17:30:54.376044 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:30:55.300079 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:30:55.300498 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-mon-a-canary-58bccfff95-98stx" not found
2024-05-19 17:30:55.536913 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-19 17:30:45 +0000 UTC LastTransitionTime:2024-05-19 17:30:45 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-19 17:30:45 +0000 UTC LastTransitionTime:2024-05-19 17:30:45 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5b697dcffd" is progressing.}] CollisionCount:<nil>}
2024-05-19 17:30:55.801793 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:30:55.802047 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-5567db46db-8fmrg" not found
2024-05-19 17:30:58.744203 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-19 17:30:45 +0000 UTC LastTransitionTime:2024-05-19 17:30:45 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-19 17:30:45 +0000 UTC LastTransitionTime:2024-05-19 17:30:45 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5b697dcffd" is progressing.}] CollisionCount:<nil>}
2024-05-19 17:30:59.428386 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-5nhh7"
2024-05-19 17:30:59.429195 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:30:59.429275 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-5nhh7
2024-05-19 17:30:59.429287 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:00.572721 D | CmdReporter: job rook-ceph-csi-detect-version has returned results
2024-05-19 17:31:00.638048 I | ceph-csi: Detected ceph CSI image version: "v3.11.0"
2024-05-19 17:31:00.666536 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-5nhh7"
2024-05-19 17:31:00.666554 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:00.666557 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:00.666558 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:00.681098 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-19 17:31:00.681132 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-19 17:31:00.681153 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-19 17:31:00.681165 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-19 17:31:00.756427 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-5nhh7"
2024-05-19 17:31:00.756476 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:00.756480 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:00.756492 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:00.774274 I | op-k8sutil: CSI_PLUGIN_TOLERATIONS="" (default)
2024-05-19 17:31:00.774511 I | op-k8sutil: CSI_PLUGIN_NODE_AFFINITY="" (default)
2024-05-19 17:31:00.774517 I | op-k8sutil: CSI_RBD_PLUGIN_TOLERATIONS="" (default)
2024-05-19 17:31:00.774533 I | op-k8sutil: CSI_RBD_PLUGIN_NODE_AFFINITY="" (default)
2024-05-19 17:31:00.774537 I | op-k8sutil: CSI_RBD_PLUGIN_RESOURCE="" (default)
2024-05-19 17:31:00.774540 I | op-k8sutil: CSI_RBD_PLUGIN_VOLUME="" (default)
2024-05-19 17:31:00.774542 I | op-k8sutil: CSI_RBD_PLUGIN_VOLUME_MOUNT="" (default)
2024-05-19 17:31:00.857176 I | op-k8sutil: CSI_RBD_PROVISIONER_TOLERATIONS="" (default)
2024-05-19 17:31:00.857328 I | op-k8sutil: CSI_RBD_PROVISIONER_NODE_AFFINITY="" (default)
2024-05-19 17:31:00.857391 I | op-k8sutil: CSI_RBD_PROVISIONER_RESOURCE="" (default)
2024-05-19 17:31:00.961780 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:31:00.966127 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-5567db46db-8fmrg" not found
2024-05-19 17:31:01.049509 I | ceph-csi: successfully started CSI Ceph RBD driver
2024-05-19 17:31:01.050124 I | op-k8sutil: CSI_CEPHFS_PLUGIN_TOLERATIONS="" (default)
2024-05-19 17:31:01.050159 I | op-k8sutil: CSI_CEPHFS_PLUGIN_NODE_AFFINITY="" (default)
2024-05-19 17:31:01.050229 I | op-k8sutil: CSI_CEPHFS_PLUGIN_RESOURCE="" (default)
2024-05-19 17:31:01.050240 I | op-k8sutil: CSI_CEPHFS_PLUGIN_VOLUME="" (default)
2024-05-19 17:31:01.050248 I | op-k8sutil: CSI_CEPHFS_PLUGIN_VOLUME_MOUNT="" (default)
2024-05-19 17:31:01.050467 D | ceph-spec: create event from a CR: "csi-rbdplugin-s226x"
2024-05-19 17:31:01.060217 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: csi-rbdplugin-s226x
2024-05-19 17:31:01.079218 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:01.079516 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:31:01.079733 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/csi-rbdplugin-s226x"
2024-05-19 17:31:01.123269 D | ceph-spec: update event from a CR: "csi-rbdplugin-s226x"
2024-05-19 17:31:01.123314 D | ceph-spec: update event on Pod %qrook-ceph/csi-rbdplugin-s226x
2024-05-19 17:31:01.123317 D | ceph-spec: wjkim : objNew.Name %scsi-rbdplugin-s226x
2024-05-19 17:31:01.123320 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:01.180296 D | ceph-spec: create event from a CR: "csi-rbdplugin-provisioner-d9b9d694c-9nsng"
2024-05-19 17:31:01.180606 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: csi-rbdplugin-provisioner-d9b9d694c-9nsng
2024-05-19 17:31:01.182589 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:01.182609 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:31:01.182628 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/csi-rbdplugin-provisioner-d9b9d694c-9nsng"
2024-05-19 17:31:01.182948 D | ceph-spec: update event from a CR: "csi-rbdplugin-s226x"
2024-05-19 17:31:01.182960 D | ceph-spec: update event on Pod %qrook-ceph/csi-rbdplugin-s226x
2024-05-19 17:31:01.182963 D | ceph-spec: wjkim : objNew.Name %scsi-rbdplugin-s226x
2024-05-19 17:31:01.182964 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:01.183333 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_TOLERATIONS="" (default)
2024-05-19 17:31:01.183346 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_NODE_AFFINITY="" (default)
2024-05-19 17:31:01.183349 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_RESOURCE="" (default)
2024-05-19 17:31:01.207439 D | ceph-spec: update event from a CR: "csi-rbdplugin-provisioner-d9b9d694c-9nsng"
2024-05-19 17:31:01.207460 D | ceph-spec: update event on Pod %qrook-ceph/csi-rbdplugin-provisioner-d9b9d694c-9nsng
2024-05-19 17:31:01.207464 D | ceph-spec: wjkim : objNew.Name %scsi-rbdplugin-provisioner-d9b9d694c-9nsng
2024-05-19 17:31:01.207465 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:01.240594 D | ceph-spec: create event from a CR: "csi-cephfsplugin-b56wj"
2024-05-19 17:31:01.240672 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: csi-cephfsplugin-b56wj
2024-05-19 17:31:01.240759 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:01.240768 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:31:01.240859 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/csi-cephfsplugin-b56wj"
2024-05-19 17:31:01.323522 I | ceph-csi: successfully started CSI CephFS driver
2024-05-19 17:31:01.323544 I | op-k8sutil: CSI_RBD_FSGROUPPOLICY="File" (configmap)
2024-05-19 17:31:01.323753 D | ceph-spec: update event from a CR: "csi-rbdplugin-provisioner-d9b9d694c-9nsng"
2024-05-19 17:31:01.323758 D | ceph-spec: update event on Pod %qrook-ceph/csi-rbdplugin-provisioner-d9b9d694c-9nsng
2024-05-19 17:31:01.323760 D | ceph-spec: wjkim : objNew.Name %scsi-rbdplugin-provisioner-d9b9d694c-9nsng
2024-05-19 17:31:01.323761 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:01.323763 D | ceph-spec: update event from a CR: "csi-cephfsplugin-b56wj"
2024-05-19 17:31:01.323764 D | ceph-spec: update event on Pod %qrook-ceph/csi-cephfsplugin-b56wj
2024-05-19 17:31:01.323765 D | ceph-spec: wjkim : objNew.Name %scsi-cephfsplugin-b56wj
2024-05-19 17:31:01.323766 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:01.419156 I | ceph-csi: CSIDriver object created for driver "rook-ceph.rbd.csi.ceph.com"
2024-05-19 17:31:01.419186 I | op-k8sutil: CSI_CEPHFS_FSGROUPPOLICY="File" (configmap)
2024-05-19 17:31:01.456262 D | ceph-spec: create event from a CR: "csi-cephfsplugin-provisioner-868bf46b56-7tnmw"
2024-05-19 17:31:01.456283 D | ceph-spec: update event from a CR: "csi-cephfsplugin-b56wj"
2024-05-19 17:31:01.456287 D | ceph-spec: update event on Pod %qrook-ceph/csi-cephfsplugin-b56wj
2024-05-19 17:31:01.456289 D | ceph-spec: wjkim : objNew.Name %scsi-cephfsplugin-b56wj
2024-05-19 17:31:01.456290 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:01.456365 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: csi-cephfsplugin-provisioner-868bf46b56-7tnmw
2024-05-19 17:31:01.456591 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:01.456605 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:31:01.456622 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/csi-cephfsplugin-provisioner-868bf46b56-7tnmw"
2024-05-19 17:31:01.458756 D | ceph-spec: update event from a CR: "csi-cephfsplugin-provisioner-868bf46b56-7tnmw"
2024-05-19 17:31:01.458811 D | ceph-spec: update event on Pod %qrook-ceph/csi-cephfsplugin-provisioner-868bf46b56-7tnmw
2024-05-19 17:31:01.458814 D | ceph-spec: wjkim : objNew.Name %scsi-cephfsplugin-provisioner-868bf46b56-7tnmw
2024-05-19 17:31:01.458816 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:01.498783 I | ceph-csi: CSIDriver object created for driver "rook-ceph.cephfs.csi.ceph.com"
2024-05-19 17:31:01.498812 I | ceph-csi: CSI NFS driver disabled
2024-05-19 17:31:01.498817 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2024-05-19 17:31:01.546251 D | op-k8sutil: removing csi-nfsplugin-provisioner deployment if it exists
2024-05-19 17:31:01.546319 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2024-05-19 17:31:01.546849 D | ceph-spec: update event from a CR: "rook-ceph-tools-67bf494bc8-pfzjm"
2024-05-19 17:31:01.546867 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-tools-67bf494bc8-pfzjm
2024-05-19 17:31:01.546870 D | ceph-spec: wjkim : objNew.Name %srook-ceph-tools-67bf494bc8-pfzjm
2024-05-19 17:31:01.546871 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:01.607646 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-5nhh7"
2024-05-19 17:31:01.607716 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:01.607720 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:01.607722 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:01.623723 D | ceph-csi: rook-ceph.nfs.csi.ceph.com CSIDriver not found; skipping deletion.
2024-05-19 17:31:01.623741 I | ceph-csi: successfully removed CSI NFS driver
2024-05-19 17:31:01.640285 D | ceph-spec: update event from a CR: "csi-cephfsplugin-provisioner-868bf46b56-7tnmw"
2024-05-19 17:31:01.640304 D | ceph-spec: update event on Pod %qrook-ceph/csi-cephfsplugin-provisioner-868bf46b56-7tnmw
2024-05-19 17:31:01.640307 D | ceph-spec: wjkim : objNew.Name %scsi-cephfsplugin-provisioner-868bf46b56-7tnmw
2024-05-19 17:31:01.640308 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:01.650671 D | ceph-csi: csi config map "rook-ceph-csi-config" (in "rook-ceph") has the expected owner; owner id: "f74a76a8-c77b-4689-9f9c-74d359dd5787"
2024-05-19 17:31:01.680324 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-19 17:31:01.684698 I | ceph-spec: parsing mon endpoints: a=10.104.237.69:6789
2024-05-19 17:31:01.684734 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc000c46810], assignment=&{Schedule:map[a:0xc000b90b40]}
2024-05-19 17:31:01.684746 D | ceph-csi: cluster "rook-ceph/my-cluster": not deploying the ceph-csi plugin holder
2024-05-19 17:31:01.684749 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-19 17:31:01.706492 I | ceph-csi: Kubernetes version is 1.30
2024-05-19 17:31:01.786491 I | ceph-csi: detecting the ceph csi image version for image "quay.io/cephcsi/cephcsi:v3.11.0"
2024-05-19 17:31:01.787587 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-19 17:30:45 +0000 UTC LastTransitionTime:2024-05-19 17:30:45 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-19 17:30:45 +0000 UTC LastTransitionTime:2024-05-19 17:30:45 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5b697dcffd" is progressing.}] CollisionCount:<nil>}
2024-05-19 17:31:01.819251 D | op-k8sutil: ConfigMap rook-ceph-csi-detect-version is already deleted
2024-05-19 17:31:01.895467 I | op-k8sutil: Removing previous job rook-ceph-csi-detect-version to start a new one
2024-05-19 17:31:01.918164 I | op-k8sutil: batch job rook-ceph-csi-detect-version still exists
2024-05-19 17:31:02.480801 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-5nhh7"
2024-05-19 17:31:02.480851 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:02.480855 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:02.480857 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:02.675806 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-5nhh7"
2024-05-19 17:31:02.675841 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:02.675846 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:02.675847 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:03.040145 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-5nhh7"
2024-05-19 17:31:03.040187 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:03.040189 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:03.040191 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:03.060481 D | ceph-spec: delete event from a CR: "rook-ceph-csi-detect-version-5nhh7"
2024-05-19 17:31:03.060860 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:03.060896 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-5nhh7" not found
2024-05-19 17:31:03.072398 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:03.072667 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-5nhh7" not found
2024-05-19 17:31:03.087636 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-19 17:31:03.088412 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:31:03.088742 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:31:03.089029 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:03.113821 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:03.134214 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-19 17:31:03.134437 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-19 17:31:03.135744 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-19 17:31:03.137943 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-5nhh7" not found
2024-05-19 17:31:03.160609 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:03.160998 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-5nhh7" not found
2024-05-19 17:31:03.203301 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:03.203399 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-5nhh7" not found
2024-05-19 17:31:03.290112 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:03.290351 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-5nhh7" not found
2024-05-19 17:31:03.457042 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:03.457699 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-5nhh7" not found
2024-05-19 17:31:03.674702 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: []. pg health: "cluster has no PGs"
2024-05-19 17:31:03.679287 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2024-05-19 17:31:03.780743 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:03.782395 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-5nhh7" not found
2024-05-19 17:31:04.423977 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:04.424113 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-5nhh7" not found
2024-05-19 17:31:04.798136 D | op-k8sutil: deployment "rook-ceph-mgr-a" status={ObservedGeneration:1 Replicas:1 UpdatedReplicas:1 ReadyReplicas:0 AvailableReplicas:0 UnavailableReplicas:1 Conditions:[{Type:Available Status:False LastUpdateTime:2024-05-19 17:30:45 +0000 UTC LastTransitionTime:2024-05-19 17:30:45 +0000 UTC Reason:MinimumReplicasUnavailable Message:Deployment does not have minimum availability.} {Type:Progressing Status:True LastUpdateTime:2024-05-19 17:30:45 +0000 UTC LastTransitionTime:2024-05-19 17:30:45 +0000 UTC Reason:ReplicaSetUpdated Message:ReplicaSet "rook-ceph-mgr-a-5b697dcffd" is progressing.}] CollisionCount:<nil>}
2024-05-19 17:31:04.921113 I | op-k8sutil: batch job rook-ceph-csi-detect-version deleted
2024-05-19 17:31:04.941726 D | ceph-spec: create event from a CR: "rook-ceph-csi-detect-version-cwphf"
2024-05-19 17:31:04.941810 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:04.941864 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:04.941867 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:31:04.941874 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-csi-detect-version-cwphf"
2024-05-19 17:31:04.954729 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-cwphf"
2024-05-19 17:31:04.955071 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:04.955087 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:04.955095 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:05.004583 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-cwphf"
2024-05-19 17:31:05.004647 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:05.004650 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:05.004651 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:05.542060 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:31:05.542215 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-mon-a-canary-58bccfff95-98stx" not found
2024-05-19 17:31:05.705900 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:05.706112 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-5nhh7" not found
2024-05-19 17:31:06.212557 D | ceph-spec: update event from a CR: "rook-ceph-mgr-a-5b697dcffd-7flbh"
2024-05-19 17:31:06.212581 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mgr-a-5b697dcffd-7flbh
2024-05-19 17:31:06.212584 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mgr-a-5b697dcffd-7flbh
2024-05-19 17:31:06.212585 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:06.322802 D | ceph-spec: update event from a CR: "rook-ceph-mgr-a-5b697dcffd-7flbh"
2024-05-19 17:31:06.322821 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-mgr-a-5b697dcffd-7flbh
2024-05-19 17:31:06.322824 D | ceph-spec: wjkim : objNew.Name %srook-ceph-mgr-a-5b697dcffd-7flbh
2024-05-19 17:31:06.322826 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:06.362237 D | ceph-spec: object "rook-ceph-mgr-a" matched on update
2024-05-19 17:31:06.362285 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:31:07.374778 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-cwphf"
2024-05-19 17:31:07.374799 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:07.374801 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:07.374804 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:07.957812 I | op-k8sutil: finished waiting for updated deployment "rook-ceph-mgr-a"
2024-05-19 17:31:08.299937 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:08.301893 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-5nhh7" not found
2024-05-19 17:31:08.304493 D | op-mgr: expected number 1 of mgrs found
2024-05-19 17:31:08.304620 D | op-k8sutil: creating service rook-ceph-mgr-dashboard
2024-05-19 17:31:08.450124 D | op-k8sutil: created service rook-ceph-mgr-dashboard
2024-05-19 17:31:08.450157 D | op-k8sutil: creating service rook-ceph-mgr
2024-05-19 17:31:08.525036 D | op-k8sutil: created service rook-ceph-mgr
2024-05-19 17:31:08.537283 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Configuring Ceph OSDs"
2024-05-19 17:31:08.537476 D | cephclient: balancer module is always 'on', doing nothingbalancer
2024-05-19 17:31:08.537512 I | op-mgr: successful modules: balancer
2024-05-19 17:31:08.538231 D | exec: Running command: ceph mgr module enable dashboard --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:08.568538 D | exec: Running command: ceph mgr module enable prometheus --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:08.568913 D | exec: Running command: ceph mgr module enable rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:08.648087 I | op-osd: start running osds in namespace "rook-ceph"
2024-05-19 17:31:08.649565 I | op-osd: wait timeout for healthy OSDs during upgrade or restart is "10m0s"
2024-05-19 17:31:08.649574 D | op-osd: no OSD migration to a new backend store is requested
2024-05-19 17:31:08.649778 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-19 17:31:08.649784 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-19 17:31:08.710348 D | op-osd: 0 of 0 OSD Deployments need update
2024-05-19 17:31:08.711054 I | op-osd: start provisioning the OSDs on PVCs, if needed
2024-05-19 17:31:08.797181 I | op-osd: no storageClassDeviceSets defined to configure OSDs on PVCs
2024-05-19 17:31:08.797417 I | op-osd: start provisioning the OSDs on nodes, if needed
2024-05-19 17:31:08.821099 I | op-osd: 1 of the 1 storage nodes are valid
2024-05-19 17:31:08.945585 I | op-osd: started OSD provisioning job for node "minikube"
2024-05-19 17:31:08.961830 I | op-osd: OSD orchestration status for node minikube is "starting"
2024-05-19 17:31:08.984723 D | ceph-spec: create event from a CR: "rook-ceph-osd-prepare-minikube-27pwt"
2024-05-19 17:31:08.984811 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:08.984865 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:08.984874 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:31:08.984885 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-osd-prepare-minikube-27pwt"
2024-05-19 17:31:09.025990 D | ceph-nodedaemon-controller: "rook-ceph-osd-prepare-minikube-27pwt" is a ceph pod!
2024-05-19 17:31:09.026118 D | ceph-nodedaemon-controller: reconciling node: "minikube"
2024-05-19 17:31:09.086353 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-19 17:31:09.091175 D | ceph-spec: update event from a CR: "rook-ceph-osd-prepare-minikube-27pwt"
2024-05-19 17:31:09.091200 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:09.091204 D | ceph-spec: wjkim : objNew.Name %srook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:09.091205 D | ceph-spec: wjkim : objNew.Name HasPrefix %vtrue
2024-05-19 17:31:09.106557 I | ceph-spec: Pod Status has changed for "rook-ceph/rook-ceph-osd-prepare-minikube-27pwt". diff=v1.PodStatus{
	Phase:      "Pending",
-	Conditions: nil,
+	Conditions: []v1.PodCondition{
+		{
+			Type:               "PodScheduled",
+			Status:             "True",
+			LastTransitionTime: s"2024-05-19 17:31:08 +0000 UTC",
+		},
+	},
	Message: "",
	Reason:  "",
	... // 12 identical fields
}
2024-05-19 17:31:09.107068 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:09.107377 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:09.107383 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:31:09.107401 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-osd-prepare-minikube-27pwt"
2024-05-19 17:31:09.156009 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube". operation: "updated"
2024-05-19 17:31:09.156100 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-19 17:31:09.160716 D | ceph-spec: update event from a CR: "rook-ceph-osd-prepare-minikube-27pwt"
2024-05-19 17:31:09.160768 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:09.160774 D | ceph-spec: wjkim : objNew.Name %srook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:09.160776 D | ceph-spec: wjkim : objNew.Name HasPrefix %vtrue
2024-05-19 17:31:09.161365 I | ceph-spec: Pod Status has changed for "rook-ceph/rook-ceph-osd-prepare-minikube-27pwt". diff=v1.PodStatus{
	Phase: "Pending",
	Conditions: []v1.PodCondition{
+		{
+			Type:               "PodReadyToStartContainers",
+			Status:             "False",
+			LastTransitionTime: s"2024-05-19 17:31:09 +0000 UTC",
+		},
+		{
+			Type:               "Initialized",
+			Status:             "False",
+			LastTransitionTime: s"2024-05-19 17:31:09 +0000 UTC",
+			Reason:             "ContainersNotInitialized",
+			Message:            "containers with incomplete status: [copy-bins]",
+		},
+		{
+			Type:               "Ready",
+			Status:             "False",
+			LastTransitionTime: s"2024-05-19 17:31:09 +0000 UTC",
+			Reason:             "ContainersNotReady",
+			Message:            "containers with unready status: [provision]",
+		},
+		{
+			Type:               "ContainersReady",
+			Status:             "False",
+			LastTransitionTime: s"2024-05-19 17:31:09 +0000 UTC",
+			Reason:             "ContainersNotReady",
+			Message:            "containers with unready status: [provision]",
+		},
		{Type: "PodScheduled", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:08 +0000 UTC"}},
	},
	Message:               "",
	Reason:                "",
	NominatedNodeName:     "",
-	HostIP:                "",
+	HostIP:                "192.168.49.2",
-	HostIPs:               nil,
+	HostIPs:               []v1.HostIP{{IP: "192.168.49.2"}},
	PodIP:                 "",
	PodIPs:                nil,
-	StartTime:             nil,
+	StartTime:             s"2024-05-19 17:31:09 +0000 UTC",
-	InitContainerStatuses: nil,
+	InitContainerStatuses: []v1.ContainerStatus{
+		{
+			Name:    "copy-bins",
+			State:   v1.ContainerState{Waiting: s"&ContainerStateWaiting{Reason:Po"...},
+			Image:   "build-f38aa6f6/ceph-amd64:latest",
+			Started: &false,
+		},
+	},
-	ContainerStatuses: nil,
+	ContainerStatuses: []v1.ContainerStatus{
+		{
+			Name:    "provision",
+			State:   v1.ContainerState{Waiting: s"&ContainerStateWaiting{Reason:Po"...},
+			Image:   "quay.io/ceph/ceph:v18",
+			Started: &false,
+		},
+	},
	QOSClass:                   "BestEffort",
	EphemeralContainerStatuses: nil,
	... // 2 identical fields
}
2024-05-19 17:31:09.161466 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:09.161545 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:09.161559 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:31:09.161577 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-osd-prepare-minikube-27pwt"
2024-05-19 17:31:09.253378 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-19 17:31:09.269196 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-19 17:31:09.814984 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-cwphf"
2024-05-19 17:31:09.815014 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:09.815019 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:09.815021 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:10.478717 W | cephclient: failed to enable mgr module "rook". trying again...
2024-05-19 17:31:10.520974 I | op-mgr: successful modules: prometheus
2024-05-19 17:31:10.880531 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-cwphf"
2024-05-19 17:31:10.880597 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:10.880602 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:10.880604 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:11.118076 D | CmdReporter: job rook-ceph-csi-detect-version has returned results
2024-05-19 17:31:11.156438 I | ceph-csi: Detected ceph CSI image version: "v3.11.0"
2024-05-19 17:31:11.190898 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-cwphf"
2024-05-19 17:31:11.190918 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:11.190920 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:11.190921 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:11.242830 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:31:11.242957 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-5567db46db-8fmrg" not found
2024-05-19 17:31:11.279790 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-19 17:31:11.279847 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-19 17:31:11.279854 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-19 17:31:11.279856 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-19 17:31:11.279864 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-cwphf"
2024-05-19 17:31:11.279868 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:11.279869 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:11.279870 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:11.617752 I | ceph-csi: successfully started CSI Ceph RBD driver
2024-05-19 17:31:11.917326 I | ceph-csi: successfully started CSI CephFS driver
2024-05-19 17:31:11.951616 I | ceph-csi: CSIDriver object updated for driver "rook-ceph.rbd.csi.ceph.com"
2024-05-19 17:31:11.991692 I | ceph-csi: CSIDriver object updated for driver "rook-ceph.cephfs.csi.ceph.com"
2024-05-19 17:31:11.991736 I | ceph-csi: CSI NFS driver disabled
2024-05-19 17:31:11.991743 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2024-05-19 17:31:12.003269 D | op-k8sutil: removing csi-nfsplugin-provisioner deployment if it exists
2024-05-19 17:31:12.003332 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2024-05-19 17:31:12.017254 D | ceph-csi: rook-ceph.nfs.csi.ceph.com CSIDriver not found; skipping deletion.
2024-05-19 17:31:12.017340 I | ceph-csi: successfully removed CSI NFS driver
2024-05-19 17:31:12.121735 D | ceph-csi: csi config map "rook-ceph-csi-config" (in "rook-ceph") has the expected owner; owner id: "f74a76a8-c77b-4689-9f9c-74d359dd5787"
2024-05-19 17:31:12.312943 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-19 17:31:12.322375 I | ceph-spec: parsing mon endpoints: a=10.104.237.69:6789
2024-05-19 17:31:12.326673 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc0016e0180], assignment=&{Schedule:map[a:0xc00169bb00]}
2024-05-19 17:31:12.326744 D | ceph-csi: cluster "rook-ceph/rook-ceph-operator-config": not deploying the ceph-csi plugin holder
2024-05-19 17:31:12.326754 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-19 17:31:12.358955 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-cwphf"
2024-05-19 17:31:12.359001 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:12.359006 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:12.359008 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:12.384479 I | ceph-csi: Kubernetes version is 1.30
2024-05-19 17:31:12.414375 I | ceph-csi: detecting the ceph csi image version for image "quay.io/cephcsi/cephcsi:v3.11.0"
2024-05-19 17:31:12.418280 D | op-k8sutil: ConfigMap rook-ceph-csi-detect-version is already deleted
2024-05-19 17:31:12.502544 D | ceph-spec: update event from a CR: "rook-ceph-osd-prepare-minikube-27pwt"
2024-05-19 17:31:12.503799 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:12.503812 D | ceph-spec: wjkim : objNew.Name %srook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:12.503814 D | ceph-spec: wjkim : objNew.Name HasPrefix %vtrue
2024-05-19 17:31:12.507235 I | ceph-spec: Pod Status has changed for "rook-ceph/rook-ceph-osd-prepare-minikube-27pwt". diff=v1.PodStatus{
	Phase: "Pending",
	Conditions: []v1.PodCondition{
		{
			Type:               "PodReadyToStartContainers",
-			Status:             "False",
+			Status:             "True",
			LastProbeTime:      {},
-			LastTransitionTime: v1.Time{Time: s"2024-05-19 17:31:09 +0000 UTC"},
+			LastTransitionTime: v1.Time{Time: s"2024-05-19 17:31:12 +0000 UTC"},
			Reason:             "",
			Message:            "",
		},
		{Type: "Initialized", Status: "False", LastTransitionTime: {Time: s"2024-05-19 17:31:09 +0000 UTC"}, Reason: "ContainersNotInitialized", ...},
		{Type: "Ready", Status: "False", LastTransitionTime: {Time: s"2024-05-19 17:31:09 +0000 UTC"}, Reason: "ContainersNotReady", ...},
		... // 2 identical elements
	},
	Message:           "",
	Reason:            "",
	NominatedNodeName: "",
	HostIP:            "192.168.49.2",
	HostIPs:           {{IP: "192.168.49.2"}},
-	PodIP:             "",
+	PodIP:             "10.244.0.16",
-	PodIPs:            nil,
+	PodIPs:            []v1.PodIP{{IP: "10.244.0.16"}},
	StartTime:         s"2024-05-19 17:31:09 +0000 UTC",
	InitContainerStatuses: []v1.ContainerStatus{
		{
			Name: "copy-bins",
			State: v1.ContainerState{
-				Waiting:    s"&ContainerStateWaiting{Reason:PodInitializing,Message:,}",
+				Waiting:    nil,
-				Running:    nil,
+				Running:    s"&ContainerStateRunning{StartedAt:2024-05-19 17:31:12 +0000 UTC,}",
				Terminated: nil,
			},
			LastTerminationState: {},
			Ready:                false,
			RestartCount:         0,
			Image:                "build-f38aa6f6/ceph-amd64:latest",
-			ImageID:              "",
+			ImageID:              "docker://sha256:da0205ef0ce8030a1ebe423bf1d7470499e4e2755a63af411e203d44eefb55e3",
-			ContainerID:          "",
+			ContainerID:          "docker://8198a5bdba74e26dd93ac042356f5872651d43aba851c328a0c8cd5b1efbacc0",
-			Started:              &false,
+			Started:              &true,
			AllocatedResources:   nil,
			Resources:            nil,
		},
	},
	ContainerStatuses: {{Name: "provision", State: {Waiting: &{Reason: "PodInitializing"}}, Image: "quay.io/ceph/ceph:v18", Started: &false, ...}},
	QOSClass:          "BestEffort",
	... // 3 identical fields
}
2024-05-19 17:31:12.507668 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:12.507808 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:12.507822 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:31:12.507848 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-osd-prepare-minikube-27pwt"
2024-05-19 17:31:12.508985 I | op-k8sutil: Removing previous job rook-ceph-csi-detect-version to start a new one
2024-05-19 17:31:12.558359 I | op-k8sutil: batch job rook-ceph-csi-detect-version still exists
2024-05-19 17:31:13.151202 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-19 17:31:13.151253 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-19 17:31:13.151270 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-19 17:31:13.243369 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-19 17:31:13.244495 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:31:13.248341 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:31:13.249943 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:13.422921 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:13.423052 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-5nhh7" not found
2024-05-19 17:31:13.501317 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-cwphf"
2024-05-19 17:31:13.501336 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:13.501339 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:13.501340 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:13.856656 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-cwphf"
2024-05-19 17:31:13.871713 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:13.872539 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:13.872552 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:14.093297 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-cwphf"
2024-05-19 17:31:14.093740 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:14.093754 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:14.093757 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:14.153142 D | ceph-spec: delete event from a CR: "rook-ceph-csi-detect-version-cwphf"
2024-05-19 17:31:14.153216 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:14.153231 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-cwphf" not found
2024-05-19 17:31:14.180532 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:14.180842 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-cwphf" not found
2024-05-19 17:31:14.198917 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:14.199092 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-cwphf" not found
2024-05-19 17:31:14.220471 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: []. pg health: "cluster has no PGs"
2024-05-19 17:31:14.221282 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:14.221532 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-cwphf" not found
2024-05-19 17:31:14.236773 D | ceph-spec: update event from a CR: "rook-ceph-osd-prepare-minikube-27pwt"
2024-05-19 17:31:14.236823 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:14.236828 D | ceph-spec: wjkim : objNew.Name %srook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:14.236830 D | ceph-spec: wjkim : objNew.Name HasPrefix %vtrue
2024-05-19 17:31:14.237333 I | ceph-spec: Pod Status has changed for "rook-ceph/rook-ceph-osd-prepare-minikube-27pwt". diff=v1.PodStatus{
	Phase: "Pending",
	Conditions: []v1.PodCondition{
		{Type: "PodReadyToStartContainers", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:12 +0000 UTC"}},
-		{
-			Type:               "Initialized",
-			Status:             "False",
-			LastTransitionTime: s"2024-05-19 17:31:09 +0000 UTC",
-			Reason:             "ContainersNotInitialized",
-			Message:            "containers with incomplete status: [copy-bins]",
-		},
+		{
+			Type:               "Initialized",
+			Status:             "True",
+			LastTransitionTime: s"2024-05-19 17:31:14 +0000 UTC",
+		},
		{Type: "Ready", Status: "False", LastTransitionTime: {Time: s"2024-05-19 17:31:09 +0000 UTC"}, Reason: "ContainersNotReady", ...},
		{Type: "ContainersReady", Status: "False", LastTransitionTime: {Time: s"2024-05-19 17:31:09 +0000 UTC"}, Reason: "ContainersNotReady", ...},
		{Type: "PodScheduled", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:08 +0000 UTC"}},
	},
	Message: "",
	Reason:  "",
	... // 4 identical fields
	PodIPs:    {{IP: "10.244.0.16"}},
	StartTime: s"2024-05-19 17:31:09 +0000 UTC",
	InitContainerStatuses: []v1.ContainerStatus{
		{
			Name: "copy-bins",
			State: v1.ContainerState{
				Waiting:    nil,
-				Running:    s"&ContainerStateRunning{StartedAt:2024-05-19 17:31:12 +0000 UTC,}",
+				Running:    nil,
-				Terminated: nil,
+				Terminated: s"&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2024-05-19 17:31:12 +0000 UTC,FinishedAt:2024-05-19 17:31:12 +0000 UTC,ContainerID:docker://8198a5bdba74e26dd93ac042356f5872651d43aba851c328a0c8cd5b1efbacc0,}",
			},
			LastTerminationState: {},
-			Ready:                false,
+			Ready:                true,
			RestartCount:         0,
			Image:                "build-f38aa6f6/ceph-amd64:latest",
			ImageID:              "docker://sha256:da0205ef0ce8030a1ebe423bf1d7470499e4e2755a63af41"...,
			ContainerID:          "docker://8198a5bdba74e26dd93ac042356f5872651d43aba851c328a0c8cd5"...,
-			Started:              &true,
+			Started:              &false,
			AllocatedResources:   nil,
			Resources:            nil,
		},
	},
	ContainerStatuses: {{Name: "provision", State: {Waiting: &{Reason: "PodInitializing"}}, Image: "quay.io/ceph/ceph:v18", Started: &false, ...}},
	QOSClass:          "BestEffort",
	... // 3 identical fields
}
2024-05-19 17:31:14.237538 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:14.237603 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:14.237608 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:31:14.237643 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-osd-prepare-minikube-27pwt"
2024-05-19 17:31:14.258264 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2024-05-19 17:31:14.265203 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:14.265992 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-cwphf" not found
2024-05-19 17:31:14.354146 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:14.361424 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-cwphf" not found
2024-05-19 17:31:14.522739 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:14.522780 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-cwphf" not found
2024-05-19 17:31:14.865604 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:14.870184 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-cwphf" not found
2024-05-19 17:31:15.281875 D | ceph-spec: update event from a CR: "csi-rbdplugin-s226x"
2024-05-19 17:31:15.281914 D | ceph-spec: update event on Pod %qrook-ceph/csi-rbdplugin-s226x
2024-05-19 17:31:15.281918 D | ceph-spec: wjkim : objNew.Name %scsi-rbdplugin-s226x
2024-05-19 17:31:15.281920 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:15.342081 D | ceph-spec: update event from a CR: "rook-ceph-osd-prepare-minikube-27pwt"
2024-05-19 17:31:15.342100 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:15.342102 D | ceph-spec: wjkim : objNew.Name %srook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:15.342104 D | ceph-spec: wjkim : objNew.Name HasPrefix %vtrue
2024-05-19 17:31:15.342543 I | ceph-spec: Pod Status has changed for "rook-ceph/rook-ceph-osd-prepare-minikube-27pwt". diff=v1.PodStatus{
-	Phase: "Pending",
+	Phase: "Running",
	Conditions: []v1.PodCondition{
		{Type: "PodReadyToStartContainers", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:12 +0000 UTC"}},
		{Type: "Initialized", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:14 +0000 UTC"}},
-		{
-			Type:               "Ready",
-			Status:             "False",
-			LastTransitionTime: s"2024-05-19 17:31:09 +0000 UTC",
-			Reason:             "ContainersNotReady",
-			Message:            "containers with unready status: [provision]",
-		},
+		{
+			Type:               "Ready",
+			Status:             "True",
+			LastTransitionTime: s"2024-05-19 17:31:15 +0000 UTC",
+		},
-		{
-			Type:               "ContainersReady",
-			Status:             "False",
-			LastTransitionTime: s"2024-05-19 17:31:09 +0000 UTC",
-			Reason:             "ContainersNotReady",
-			Message:            "containers with unready status: [provision]",
-		},
+		{
+			Type:               "ContainersReady",
+			Status:             "True",
+			LastTransitionTime: s"2024-05-19 17:31:15 +0000 UTC",
+		},
		{Type: "PodScheduled", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:08 +0000 UTC"}},
	},
	Message: "",
	Reason:  "",
	... // 5 identical fields
	StartTime:             s"2024-05-19 17:31:09 +0000 UTC",
	InitContainerStatuses: {{Name: "copy-bins", State: {Terminated: &{Reason: "Completed", StartedAt: {Time: s"2024-05-19 17:31:12 +0000 UTC"}, FinishedAt: {Time: s"2024-05-19 17:31:12 +0000 UTC"}, ContainerID: "docker://8198a5bdba74e26dd93ac042356f5872651d43aba851c328a0c8cd5"...}}, Ready: true, Image: "build-f38aa6f6/ceph-amd64:latest", ...}},
	ContainerStatuses: []v1.ContainerStatus{
		{
			Name: "provision",
			State: v1.ContainerState{
-				Waiting:    s"&ContainerStateWaiting{Reason:PodInitializing,Message:,}",
+				Waiting:    nil,
-				Running:    nil,
+				Running:    s"&ContainerStateRunning{StartedAt:2024-05-19 17:31:15 +0000 UTC,}",
				Terminated: nil,
			},
			LastTerminationState: {},
-			Ready:                false,
+			Ready:                true,
			RestartCount:         0,
			Image:                "quay.io/ceph/ceph:v18",
-			ImageID:              "",
+			ImageID:              "docker-pullable://quay.io/ceph/ceph@sha256:6d6642f8c5b824b896ad5b1574e0c49b7807098bb9b946067e7bd2f03adf5b6e",
-			ContainerID:          "",
+			ContainerID:          "docker://332283ee52e5e46829296324a5f0af268c87c54118dd0c1a88357d53973741fc",
-			Started:              &false,
+			Started:              &true,
			AllocatedResources:   nil,
			Resources:            nil,
		},
	},
	QOSClass:                   "BestEffort",
	EphemeralContainerStatuses: nil,
	... // 2 identical fields
}
2024-05-19 17:31:15.342601 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:15.342642 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:15.342649 E | nvmeofosd-controller: other status: Running
2024-05-19 17:31:15.342662 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-osd-prepare-minikube-27pwt"
2024-05-19 17:31:15.361924 D | ceph-spec: object "rook-ceph-osd-minikube-status" matched on update
2024-05-19 17:31:15.361951 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-status"
2024-05-19 17:31:15.375942 I | op-osd: OSD orchestration status for node minikube is "orchestrating"
2024-05-19 17:31:15.479668 D | exec: Running command: ceph mgr module enable rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:15.530218 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:15.531419 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-cwphf" not found
2024-05-19 17:31:15.611530 I | op-k8sutil: batch job rook-ceph-csi-detect-version deleted
2024-05-19 17:31:15.665746 D | ceph-spec: create event from a CR: "rook-ceph-csi-detect-version-kwcwk"
2024-05-19 17:31:15.665835 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:15.665877 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:15.665879 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:31:15.665887 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-csi-detect-version-kwcwk"
2024-05-19 17:31:15.686782 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-kwcwk"
2024-05-19 17:31:15.686810 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:15.686814 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:15.686816 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:15.802153 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-kwcwk"
2024-05-19 17:31:15.802255 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:15.802271 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:15.802273 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:16.335231 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-19 17:31:16.335759 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-19 17:31:16.413085 D | ceph-spec: update event from a CR: "csi-cephfsplugin-b56wj"
2024-05-19 17:31:16.413101 D | ceph-spec: update event on Pod %qrook-ceph/csi-cephfsplugin-b56wj
2024-05-19 17:31:16.413103 D | ceph-spec: wjkim : objNew.Name %scsi-cephfsplugin-b56wj
2024-05-19 17:31:16.413105 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:16.422390 W | cephclient: failed to enable mgr module "rook". trying again...
2024-05-19 17:31:16.635141 I | op-mgr: setting ceph dashboard "admin" login creds
2024-05-19 17:31:16.662362 D | exec: Running command: ceph dashboard ac-user-create admin -i /tmp/2648630293 administrator --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:16.820794 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:16.821428 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-cwphf" not found
2024-05-19 17:31:17.432802 I | cephclient: command failed for create dashboard user. trying again...
2024-05-19 17:31:18.757511 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-19 17:31:18.757533 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-19 17:31:18.924631 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-kwcwk"
2024-05-19 17:31:18.924663 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:18.924665 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:18.924666 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:19.400917 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:19.401010 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-cwphf" not found
2024-05-19 17:31:20.239015 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-kwcwk"
2024-05-19 17:31:20.239038 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:20.239042 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:20.239044 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:21.221287 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-kwcwk"
2024-05-19 17:31:21.221307 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:21.221311 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:21.221312 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:21.382365 D | CmdReporter: job rook-ceph-csi-detect-version has returned results
2024-05-19 17:31:21.434653 D | exec: Running command: ceph mgr module enable rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:21.456442 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-kwcwk"
2024-05-19 17:31:21.456476 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:21.456478 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:21.456480 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:21.509491 I | ceph-csi: Detected ceph CSI image version: "v3.11.0"
2024-05-19 17:31:21.526892 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-kwcwk"
2024-05-19 17:31:21.526912 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:21.526917 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:21.526919 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:21.612751 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-19 17:31:21.612793 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-19 17:31:21.612800 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-19 17:31:21.612803 D | ceph-spec: object "rook-ceph-csi-detect-version" did not match on delete
2024-05-19 17:31:21.844736 I | ceph-csi: successfully started CSI Ceph RBD driver
2024-05-19 17:31:22.079262 I | ceph-csi: successfully started CSI CephFS driver
2024-05-19 17:31:22.131169 I | ceph-csi: CSIDriver object updated for driver "rook-ceph.rbd.csi.ceph.com"
2024-05-19 17:31:22.184667 I | ceph-csi: CSIDriver object updated for driver "rook-ceph.cephfs.csi.ceph.com"
2024-05-19 17:31:22.184924 I | ceph-csi: CSI NFS driver disabled
2024-05-19 17:31:22.185020 I | op-k8sutil: removing daemonset csi-nfsplugin if it exists
2024-05-19 17:31:22.197439 D | op-k8sutil: removing csi-nfsplugin-provisioner deployment if it exists
2024-05-19 17:31:22.197456 I | op-k8sutil: removing deployment csi-nfsplugin-provisioner if it exists
2024-05-19 17:31:22.398138 D | ceph-csi: rook-ceph.nfs.csi.ceph.com CSIDriver not found; skipping deletion.
2024-05-19 17:31:22.398178 I | ceph-csi: successfully removed CSI NFS driver
2024-05-19 17:31:22.435025 D | exec: Running command: ceph dashboard ac-user-create admin -i /tmp/2648630293 administrator --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:22.586794 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-kwcwk"
2024-05-19 17:31:22.586835 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:22.586839 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:22.586841 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:22.831234 W | cephclient: failed to enable mgr module "rook". trying again...
2024-05-19 17:31:23.154108 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-19 17:31:23.154217 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-19 17:31:23.154237 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-19 17:31:23.158587 I | cephclient: command failed for create dashboard user. trying again...
2024-05-19 17:31:23.685168 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:23.706292 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-5nhh7" not found
2024-05-19 17:31:24.566243 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:24.603838 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-cwphf" not found
2024-05-19 17:31:24.782832 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-kwcwk"
2024-05-19 17:31:24.782862 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:24.782865 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:24.782866 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:25.829349 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-kwcwk"
2024-05-19 17:31:25.829394 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:25.829398 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:25.829400 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:25.925681 D | ceph-spec: update event from a CR: "rook-ceph-csi-detect-version-kwcwk"
2024-05-19 17:31:25.925736 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:25.925739 D | ceph-spec: wjkim : objNew.Name %srook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:25.925740 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:25.938781 D | ceph-spec: delete event from a CR: "rook-ceph-csi-detect-version-kwcwk"
2024-05-19 17:31:25.938836 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:25.938848 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-kwcwk" not found
2024-05-19 17:31:25.949831 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:25.987726 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-kwcwk" not found
2024-05-19 17:31:26.013335 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:26.013379 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-kwcwk" not found
2024-05-19 17:31:26.031933 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:31:26.032698 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-mon-a-canary-58bccfff95-98stx" not found
2024-05-19 17:31:26.034734 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:26.034829 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-kwcwk" not found
2024-05-19 17:31:26.082661 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:26.082943 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-kwcwk" not found
2024-05-19 17:31:26.166063 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:26.166955 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-kwcwk" not found
2024-05-19 17:31:26.335983 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:26.336211 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-kwcwk" not found
2024-05-19 17:31:26.657988 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:26.658356 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-kwcwk" not found
2024-05-19 17:31:27.001404 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-detect-version-2lnnf
2024-05-19 17:31:27.001550 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-detect-version-2lnnf" not found
2024-05-19 17:31:27.299422 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:27.299694 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-kwcwk" not found
2024-05-19 17:31:27.837272 D | exec: Running command: ceph mgr module enable rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:28.164931 D | exec: Running command: ceph dashboard ac-user-create admin -i /tmp/2648630293 administrator --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:28.584788 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:28.585419 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-kwcwk" not found
2024-05-19 17:31:28.846281 I | cephclient: command failed for create dashboard user. trying again...
2024-05-19 17:31:29.516018 I | op-mgr: successful modules: mgr module(s) from the spec
2024-05-19 17:31:29.516233 D | exec: Running command: ceph mgr module enable rook --force --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:30.542001 D | exec: Running command: ceph orch set backend rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:31.147125 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:31.147492 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-kwcwk" not found
2024-05-19 17:31:31.381744 I | cephclient: command failed for set rook backend. trying again...
2024-05-19 17:31:31.543952 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-19 17:31:31.544104 D | ceph-cluster-controller: node watcher: cluster "rook-ceph" is not ready. skipping orchestration
2024-05-19 17:31:31.749473 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:31:31.750102 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-5567db46db-8fmrg" not found
2024-05-19 17:31:33.159827 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-19 17:31:33.160069 D | ceph-spec: "ceph-block-pool-controller": CephCluster "rook-ceph/builtin-mgr" initial reconcile is not complete yet...
2024-05-19 17:31:33.160115 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-19 17:31:33.679963 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-19 17:31:33.683081 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:31:33.683466 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:31:33.697783 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:33.698991 I | op-osd: OSD orchestration status for node minikube is "completed"
2024-05-19 17:31:33.699067 I | op-osd: creating OSD 0 on node "minikube"
2024-05-19 17:31:33.699256 D | ceph-spec: object "rook-ceph-osd-minikube-status" matched on update
2024-05-19 17:31:33.701429 D | ceph-spec: do not reconcile on configmap "rook-ceph-osd-minikube-status"
2024-05-19 17:31:33.704558 D | op-k8sutil: duplicate env var "POD_NAMESPACE" skipped on container "osd"
2024-05-19 17:31:33.704580 D | op-k8sutil: duplicate env var "NODE_NAME" skipped on container "osd"
2024-05-19 17:31:33.704647 D | ceph-spec: CephCluster "rook-ceph" status: "Progressing". "Processing OSD 0 on node \"minikube\""
2024-05-19 17:31:33.808519 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-19 17:31:33.808544 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-19 17:31:33.866940 D | exec: Running command: ceph dashboard ac-user-create admin -i /tmp/2648630293 administrator --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:33.890192 D | ceph-spec: object "rook-ceph-osd-minikube-status" did not match on delete
2024-05-19 17:31:33.890287 D | ceph-spec: object "rook-ceph-osd-minikube-status" did not match on delete
2024-05-19 17:31:33.890291 D | ceph-spec: do not reconcile on "rook-ceph-osd-minikube-status" config map changes
2024-05-19 17:31:33.890306 D | ceph-spec: object "rook-ceph-osd-minikube-status" did not match on delete
2024-05-19 17:31:33.926184 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:33.978551 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-05-19 17:31:33.978611 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:31:33.978623 D | ceph-spec: create event from a CR: "rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:31:33.979175 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:33.979788 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:33.979809 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:31:33.979888 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:31:34.061759 D | ceph-nodedaemon-controller: "rook-ceph-osd-0-7f5bb7dcb9-v2tzl" is a ceph pod!
2024-05-19 17:31:34.062009 D | ceph-nodedaemon-controller: reconciling node: "minikube"
2024-05-19 17:31:34.069416 D | ceph-spec: update event from a CR: "rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:31:34.071140 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:34.071170 D | ceph-spec: wjkim : objNew.Name %srook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:34.071174 D | ceph-spec: wjkim : objNew.Name HasPrefix %vtrue
2024-05-19 17:31:34.072076 I | ceph-spec: Pod Status has changed for "rook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl". diff=v1.PodStatus{
	Phase:      "Pending",
-	Conditions: nil,
+	Conditions: []v1.PodCondition{
+		{
+			Type:               "PodScheduled",
+			Status:             "True",
+			LastTransitionTime: s"2024-05-19 17:31:33 +0000 UTC",
+		},
+	},
	Message: "",
	Reason:  "",
	... // 12 identical fields
}
2024-05-19 17:31:34.072415 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:34.072772 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:34.072781 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:31:34.072842 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:31:34.110781 D | ceph-spec: update event from a CR: "rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:31:34.110815 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:34.110819 D | ceph-spec: wjkim : objNew.Name %srook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:34.110821 D | ceph-spec: wjkim : objNew.Name HasPrefix %vtrue
2024-05-19 17:31:34.111387 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-19 17:31:34.115172 I | ceph-spec: Pod Status has changed for "rook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl". diff=v1.PodStatus{
	Phase: "Pending",
	Conditions: []v1.PodCondition{
+		{
+			Type:               "PodReadyToStartContainers",
+			Status:             "False",
+			LastTransitionTime: s"2024-05-19 17:31:33 +0000 UTC",
+		},
+		{
+			Type:               "Initialized",
+			Status:             "False",
+			LastTransitionTime: s"2024-05-19 17:31:33 +0000 UTC",
+			Reason:             "ContainersNotInitialized",
+			Message:            "containers with incomplete status: [activate expand-bluefs chown-container-data-dir]",
+		},
+		{
+			Type:               "Ready",
+			Status:             "False",
+			LastTransitionTime: s"2024-05-19 17:31:33 +0000 UTC",
+			Reason:             "ContainersNotReady",
+			Message:            "containers with unready status: [osd]",
+		},
+		{
+			Type:               "ContainersReady",
+			Status:             "False",
+			LastTransitionTime: s"2024-05-19 17:31:33 +0000 UTC",
+			Reason:             "ContainersNotReady",
+			Message:            "containers with unready status: [osd]",
+		},
		{Type: "PodScheduled", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:33 +0000 UTC"}},
	},
	Message:               "",
	Reason:                "",
	NominatedNodeName:     "",
-	HostIP:                "",
+	HostIP:                "192.168.49.2",
-	HostIPs:               nil,
+	HostIPs:               []v1.HostIP{{IP: "192.168.49.2"}},
	PodIP:                 "",
	PodIPs:                nil,
-	StartTime:             nil,
+	StartTime:             s"2024-05-19 17:31:33 +0000 UTC",
-	InitContainerStatuses: nil,
+	InitContainerStatuses: []v1.ContainerStatus{
+		{
+			Name:    "activate",
+			State:   v1.ContainerState{Waiting: s"&ContainerStateWaiting{Reason:Po"...},
+			Image:   "quay.io/ceph/ceph:v18",
+			Started: &false,
+		},
+		{
+			Name:    "expand-bluefs",
+			State:   v1.ContainerState{Waiting: s"&ContainerStateWaiting{Reason:Po"...},
+			Image:   "quay.io/ceph/ceph:v18",
+			Started: &false,
+		},
+		{
+			Name:    "chown-container-data-dir",
+			State:   v1.ContainerState{Waiting: s"&ContainerStateWaiting{Reason:Po"...},
+			Image:   "quay.io/ceph/ceph:v18",
+			Started: &false,
+		},
+	},
-	ContainerStatuses: nil,
+	ContainerStatuses: []v1.ContainerStatus{
+		{
+			Name:    "osd",
+			State:   v1.ContainerState{Waiting: s"&ContainerStateWaiting{Reason:Po"...},
+			Image:   "quay.io/ceph/ceph:v18",
+			Started: &false,
+		},
+	},
	QOSClass:                   "BestEffort",
	EphemeralContainerStatuses: nil,
	... // 2 identical fields
}
2024-05-19 17:31:34.115459 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:34.115775 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:34.115796 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:31:34.115902 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:31:34.243575 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-05-19 17:31:34.243599 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:31:34.244585 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube". operation: "updated"
2024-05-19 17:31:34.244601 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-19 17:31:34.244804 D | ceph-spec: object "rook-ceph-exporter-minikube" matched on update
2024-05-19 17:31:34.244813 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:31:34.384325 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-05-19 17:31:34.384404 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:31:34.450000 D | ceph-spec: object "rook-ceph-exporter-minikube" matched on update
2024-05-19 17:31:34.450031 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:31:34.703404 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-19 17:31:34.761207 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-595c5f6b68-bksvk"
2024-05-19 17:31:34.761513 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:34.761573 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:34.761586 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:35.013435 D | ceph-spec: update event from a CR: "rook-ceph-osd-prepare-minikube-27pwt"
2024-05-19 17:31:35.013516 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:35.013523 D | ceph-spec: wjkim : objNew.Name %srook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:35.013526 D | ceph-spec: wjkim : objNew.Name HasPrefix %vtrue
2024-05-19 17:31:35.025326 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:35.026239 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-cwphf" not found
2024-05-19 17:31:35.031448 I | ceph-spec: Pod Status has changed for "rook-ceph/rook-ceph-osd-prepare-minikube-27pwt". diff=v1.PodStatus{
	Phase: "Running",
	Conditions: []v1.PodCondition{
		{Type: "PodReadyToStartContainers", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:12 +0000 UTC"}},
		{
			... // 2 identical fields
			LastProbeTime:      {},
			LastTransitionTime: {Time: s"2024-05-19 17:31:14 +0000 UTC"},
-			Reason:             "",
+			Reason:             "PodCompleted",
			Message:            "",
		},
		{
			Type:               "Ready",
-			Status:             "True",
+			Status:             "False",
			LastProbeTime:      {},
-			LastTransitionTime: v1.Time{Time: s"2024-05-19 17:31:15 +0000 UTC"},
+			LastTransitionTime: v1.Time{Time: s"2024-05-19 17:31:34 +0000 UTC"},
-			Reason:             "",
+			Reason:             "PodCompleted",
			Message:            "",
		},
		{
			Type:               "ContainersReady",
-			Status:             "True",
+			Status:             "False",
			LastProbeTime:      {},
-			LastTransitionTime: v1.Time{Time: s"2024-05-19 17:31:15 +0000 UTC"},
+			LastTransitionTime: v1.Time{Time: s"2024-05-19 17:31:34 +0000 UTC"},
-			Reason:             "",
+			Reason:             "PodCompleted",
			Message:            "",
		},
		{Type: "PodScheduled", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:08 +0000 UTC"}},
	},
	Message: "",
	Reason:  "",
	... // 5 identical fields
	StartTime:             s"2024-05-19 17:31:09 +0000 UTC",
	InitContainerStatuses: {{Name: "copy-bins", State: {Terminated: &{Reason: "Completed", StartedAt: {Time: s"2024-05-19 17:31:12 +0000 UTC"}, FinishedAt: {Time: s"2024-05-19 17:31:12 +0000 UTC"}, ContainerID: "docker://8198a5bdba74e26dd93ac042356f5872651d43aba851c328a0c8cd5"...}}, Ready: true, Image: "build-f38aa6f6/ceph-amd64:latest", ...}},
	ContainerStatuses: []v1.ContainerStatus{
		{
			Name: "provision",
			State: v1.ContainerState{
				Waiting:    nil,
-				Running:    s"&ContainerStateRunning{StartedAt:2024-05-19 17:31:15 +0000 UTC,}",
+				Running:    nil,
-				Terminated: nil,
+				Terminated: s"&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2024-05-19 17:31:15 +0000 UTC,FinishedAt:2024-05-19 17:31:33 +0000 UTC,ContainerID:docker://332283ee52e5e46829296324a5f0af268c87c54118dd0c1a88357d53973741fc,}",
			},
			LastTerminationState: {},
-			Ready:                true,
+			Ready:                false,
			RestartCount:         0,
			Image:                "quay.io/ceph/ceph:v18",
			ImageID:              "docker-pullable://quay.io/ceph/ceph@sha256:6d6642f8c5b824b896ad5"...,
			ContainerID:          "docker://332283ee52e5e46829296324a5f0af268c87c54118dd0c1a88357d5"...,
-			Started:              &true,
+			Started:              &false,
			AllocatedResources:   nil,
			Resources:            nil,
		},
	},
	QOSClass:                   "BestEffort",
	EphemeralContainerStatuses: nil,
	... // 2 identical fields
}
2024-05-19 17:31:35.031581 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:35.031880 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:35.031886 E | nvmeofosd-controller: other status: Running
2024-05-19 17:31:35.031946 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-osd-prepare-minikube-27pwt"
2024-05-19 17:31:35.099351 D | ceph-spec: object "rook-ceph-exporter-minikube" matched on update
2024-05-19 17:31:35.099603 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:31:35.143582 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-19 17:31:35.241065 D | ceph-spec: object "rook-ceph-exporter-minikube" matched on update
2024-05-19 17:31:35.241086 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:31:35.560396 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":2}}
2024-05-19 17:31:35.588703 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":2}}
2024-05-19 17:31:35.617469 D | op-osd: successfully deleted key rotation cron jobs
2024-05-19 17:31:35.617501 D | exec: Running command: ceph osd crush class ls --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:35.910797 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: []. pg health: "cluster has no PGs"
2024-05-19 17:31:35.915590 I | clusterdisruption-controller: reconciling osd pdb reconciler as the allowed disruptions in default pdb is 0
2024-05-19 17:31:35.915676 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-19 17:31:35.915709 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:31:35.915736 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:31:35.915847 I | clusterdisruption-controller: osd "rook-ceph-osd-0" is down but no node drain is detected
2024-05-19 17:31:35.915863 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:36.273465 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:36.274069 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-kwcwk" not found
2024-05-19 17:31:36.336249 I | cephclient: command failed for create dashboard user. trying again...
2024-05-19 17:31:36.382527 D | exec: Running command: ceph orch set backend rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:36.808978 I | op-osd: finished running OSDs in namespace "rook-ceph"
2024-05-19 17:31:36.809029 I | ceph-cluster-controller: done reconciling ceph cluster in namespace "rook-ceph"
2024-05-19 17:31:36.809243 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2024-05-19 17:31:36.819081 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-19 17:31:36.822583 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-19 17:31:36.830758 E | ceph-spec: failed to update cluster condition to {Type:Ready Status:True Reason:ClusterCreated Message:Cluster created successfully LastHeartbeatTime:2024-05-19 17:31:36.809235363 +0000 UTC m=+144.879914228 LastTransitionTime:2024-05-19 17:31:36.809235193 +0000 UTC m=+144.879914105}. failed to update object "rook-ceph/my-cluster" status: Operation cannot be fulfilled on cephclusters.ceph.rook.io "my-cluster": the object has been modified; please apply your changes to the latest version and try again
2024-05-19 17:31:36.830780 D | ceph-csi: using "rook-ceph" for csi configmap namespace
2024-05-19 17:31:36.830868 I | ceph-cluster-controller: reporting cluster telemetry
2024-05-19 17:31:36.830878 D | op-config: setting "rook/version"="1.2.3" option in the mon config-key store
2024-05-19 17:31:36.830884 D | exec: Running command: ceph config-key set rook/version 1.2.3 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:36.835600 I | ceph-cluster-controller: enabling ceph mon monitoring goroutine for cluster "rook-ceph"
2024-05-19 17:31:36.835828 I | op-osd: ceph osd status in namespace "rook-ceph" check interval "5s"
2024-05-19 17:31:36.835837 I | ceph-cluster-controller: enabling ceph osd monitoring goroutine for cluster "rook-ceph"
2024-05-19 17:31:36.835844 I | ceph-cluster-controller: enabling ceph status monitoring goroutine for cluster "rook-ceph"
2024-05-19 17:31:36.835981 D | ceph-cluster-controller: successfully configured CephCluster "rook-ceph/my-cluster"
2024-05-19 17:31:36.836113 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "30s"
2024-05-19 17:31:36.836158 D | ceph-cluster-controller: checking health of cluster
2024-05-19 17:31:36.836197 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2024-05-19 17:31:36.954657 I | clusterdisruption-controller: osd is down in failure domain "minikube". pg health: "cluster has no PGs"
2024-05-19 17:31:36.956473 D | clusterdisruption-controller: deleting default pdb with maxUnavailable=1 for all osd
2024-05-19 17:31:36.956526 I | clusterdisruption-controller: deleting the default pdb "rook-ceph-osd" with maxUnavailable=1 for all osd
2024-05-19 17:31:37.320210 D | ceph-spec: update event from a CR: "rook-ceph-osd-prepare-minikube-27pwt"
2024-05-19 17:31:37.320234 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:37.320238 D | ceph-spec: wjkim : objNew.Name %srook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:37.320239 D | ceph-spec: wjkim : objNew.Name HasPrefix %vtrue
2024-05-19 17:31:37.320659 I | ceph-spec: Pod Status has changed for "rook-ceph/rook-ceph-osd-prepare-minikube-27pwt". diff=v1.PodStatus{
-	Phase: "Running",
+	Phase: "Succeeded",
	Conditions: []v1.PodCondition{
		{
			Type:               "PodReadyToStartContainers",
-			Status:             "True",
+			Status:             "False",
			LastProbeTime:      {},
-			LastTransitionTime: v1.Time{Time: s"2024-05-19 17:31:12 +0000 UTC"},
+			LastTransitionTime: v1.Time{Time: s"2024-05-19 17:31:37 +0000 UTC"},
			Reason:             "",
			Message:            "",
		},
		{Type: "Initialized", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:14 +0000 UTC"}, Reason: "PodCompleted", ...},
		{Type: "Ready", Status: "False", LastTransitionTime: {Time: s"2024-05-19 17:31:34 +0000 UTC"}, Reason: "PodCompleted", ...},
		... // 2 identical elements
	},
	Message:               "",
	Reason:                "",
	NominatedNodeName:     "",
	HostIP:                "192.168.49.2",
	HostIPs:               {{IP: "192.168.49.2"}},
-	PodIP:                 "10.244.0.16",
+	PodIP:                 "",
-	PodIPs:                []v1.PodIP{{IP: "10.244.0.16"}},
+	PodIPs:                nil,
	StartTime:             s"2024-05-19 17:31:09 +0000 UTC",
	InitContainerStatuses: {{Name: "copy-bins", State: {Terminated: &{Reason: "Completed", StartedAt: {Time: s"2024-05-19 17:31:12 +0000 UTC"}, FinishedAt: {Time: s"2024-05-19 17:31:12 +0000 UTC"}, ContainerID: "docker://8198a5bdba74e26dd93ac042356f5872651d43aba851c328a0c8cd5"...}}, Ready: true, Image: "build-f38aa6f6/ceph-amd64:latest", ...}},
	... // 5 identical fields
}
2024-05-19 17:31:37.320753 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:37.320793 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:37.320795 E | nvmeofosd-controller: other status: Succeeded
2024-05-19 17:31:37.320838 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-osd-prepare-minikube-27pwt"
2024-05-19 17:31:37.437770 D | ceph-spec: update event from a CR: "rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:31:37.437800 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:37.438479 D | ceph-spec: wjkim : objNew.Name %srook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:37.438481 D | ceph-spec: wjkim : objNew.Name HasPrefix %vtrue
2024-05-19 17:31:37.439219 I | ceph-spec: Pod Status has changed for "rook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl". diff=v1.PodStatus{
	Phase: "Pending",
	Conditions: []v1.PodCondition{
		{
			Type:               "PodReadyToStartContainers",
-			Status:             "False",
+			Status:             "True",
			LastProbeTime:      {},
-			LastTransitionTime: v1.Time{Time: s"2024-05-19 17:31:33 +0000 UTC"},
+			LastTransitionTime: v1.Time{Time: s"2024-05-19 17:31:37 +0000 UTC"},
			Reason:             "",
			Message:            "",
		},
		{Type: "Initialized", Status: "False", LastTransitionTime: {Time: s"2024-05-19 17:31:33 +0000 UTC"}, Reason: "ContainersNotInitialized", ...},
		{Type: "Ready", Status: "False", LastTransitionTime: {Time: s"2024-05-19 17:31:33 +0000 UTC"}, Reason: "ContainersNotReady", ...},
		... // 2 identical elements
	},
	Message:           "",
	Reason:            "",
	NominatedNodeName: "",
	HostIP:            "192.168.49.2",
	HostIPs:           {{IP: "192.168.49.2"}},
-	PodIP:             "",
+	PodIP:             "10.244.0.18",
-	PodIPs:            nil,
+	PodIPs:            []v1.PodIP{{IP: "10.244.0.18"}},
	StartTime:         s"2024-05-19 17:31:33 +0000 UTC",
	InitContainerStatuses: []v1.ContainerStatus{
		{
			Name: "activate",
			State: v1.ContainerState{
-				Waiting:    s"&ContainerStateWaiting{Reason:PodInitializing,Message:,}",
+				Waiting:    nil,
-				Running:    nil,
+				Running:    s"&ContainerStateRunning{StartedAt:2024-05-19 17:31:37 +0000 UTC,}",
				Terminated: nil,
			},
			LastTerminationState: {},
			Ready:                false,
			RestartCount:         0,
			Image:                "quay.io/ceph/ceph:v18",
-			ImageID:              "",
+			ImageID:              "docker-pullable://quay.io/ceph/ceph@sha256:6d6642f8c5b824b896ad5b1574e0c49b7807098bb9b946067e7bd2f03adf5b6e",
-			ContainerID:          "",
+			ContainerID:          "docker://05621d7afad48a5120dd42a9db40bbd8bcf5bba44d314863700facac03b081f7",
-			Started:              &false,
+			Started:              &true,
			AllocatedResources:   nil,
			Resources:            nil,
		},
		{Name: "expand-bluefs", State: {Waiting: &{Reason: "PodInitializing"}}, Image: "quay.io/ceph/ceph:v18", Started: &false, ...},
		{Name: "chown-container-data-dir", State: {Waiting: &{Reason: "PodInitializing"}}, Image: "quay.io/ceph/ceph:v18", Started: &false, ...},
	},
	ContainerStatuses: {{Name: "osd", State: {Waiting: &{Reason: "PodInitializing"}}, Image: "quay.io/ceph/ceph:v18", Started: &false, ...}},
	QOSClass:          "BestEffort",
	... // 3 identical fields
}
2024-05-19 17:31:37.439623 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:37.439767 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:37.439775 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:31:37.439824 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:31:37.598997 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:889ac511-7d79-4ca4-921d-b4624254e96b ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:2 NumOsd:1 NumUpOsd:0 NumInOsd:1 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:0 AvailableBytes:0 TotalBytes:0 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:false Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-05-19 17:31:37.618326 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:37.695777 D | telemetry: set telemetry key: rook/version=1.2.3
2024-05-19 17:31:37.697783 D | op-config: setting "rook/kubernetes/version"="v1.30.0" option in the mon config-key store
2024-05-19 17:31:37.697801 D | exec: Running command: ceph config-key set rook/kubernetes/version v1.30.0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:37.896840 I | cephclient: command failed for set rook backend. trying again...
2024-05-19 17:31:38.387416 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-595c5f6b68-bksvk"
2024-05-19 17:31:38.387611 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:38.387634 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:38.387636 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:38.438969 D | ceph-spec: update event from a CR: "rook-ceph-osd-prepare-minikube-27pwt"
2024-05-19 17:31:38.439948 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:38.440198 D | ceph-spec: wjkim : objNew.Name %srook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:38.440322 D | ceph-spec: wjkim : objNew.Name HasPrefix %vtrue
2024-05-19 17:31:38.505340 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-595c5f6b68-bksvk"
2024-05-19 17:31:38.505356 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:38.505358 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:38.505359 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:38.560168 D | ceph-spec: update event from a CR: "rook-ceph-osd-prepare-minikube-27pwt"
2024-05-19 17:31:38.561418 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:38.561482 D | ceph-spec: wjkim : objNew.Name %srook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:38.561522 D | ceph-spec: wjkim : objNew.Name HasPrefix %vtrue
2024-05-19 17:31:38.561874 I | ceph-spec: Pod Status has changed for "rook-ceph/rook-ceph-osd-prepare-minikube-27pwt". diff=v1.PodStatus{
	... // 5 identical fields
	HostIP:                "192.168.49.2",
	HostIPs:               {{IP: "192.168.49.2"}},
-	PodIP:                 "",
+	PodIP:                 "10.244.0.16",
-	PodIPs:                nil,
+	PodIPs:                []v1.PodIP{{IP: "10.244.0.16"}},
	StartTime:             s"2024-05-19 17:31:09 +0000 UTC",
	InitContainerStatuses: {{Name: "copy-bins", State: {Terminated: &{Reason: "Completed", StartedAt: {Time: s"2024-05-19 17:31:12 +0000 UTC"}, FinishedAt: {Time: s"2024-05-19 17:31:12 +0000 UTC"}, ContainerID: "docker://8198a5bdba74e26dd93ac042356f5872651d43aba851c328a0c8cd5"...}}, Ready: true, Image: "build-f38aa6f6/ceph-amd64:latest", ...}},
	... // 5 identical fields
}
2024-05-19 17:31:38.573050 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-prepare-minikube-27pwt
2024-05-19 17:31:38.573575 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:38.573581 E | nvmeofosd-controller: other status: Succeeded
2024-05-19 17:31:38.573671 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-osd-prepare-minikube-27pwt"
2024-05-19 17:31:38.606143 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-595c5f6b68-bksvk"
2024-05-19 17:31:38.606272 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:38.606282 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:38.606284 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:38.658460 D | ceph-nodedaemon-controller: "rook-ceph-exporter-minikube-595c5f6b68-bksvk" is a ceph pod!
2024-05-19 17:31:38.658927 D | ceph-nodedaemon-controller: reconciling node: "minikube"
2024-05-19 17:31:38.659130 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-19 17:31:38.659691 D | ceph-spec: delete event from a CR: "rook-ceph-exporter-minikube-595c5f6b68-bksvk"
2024-05-19 17:31:38.659732 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:38.659943 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-595c5f6b68-bksvk" not found
2024-05-19 17:31:38.674887 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:38.753369 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-595c5f6b68-bksvk" not found
2024-05-19 17:31:38.757826 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube". operation: "updated"
2024-05-19 17:31:38.757891 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-19 17:31:38.795015 D | ceph-spec: object "rook-ceph-exporter-minikube" matched on update
2024-05-19 17:31:38.795034 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:31:38.796513 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:38.796546 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-595c5f6b68-bksvk" not found
2024-05-19 17:31:38.798587 D | ceph-spec: create event from a CR: "rook-ceph-exporter-minikube-5567db46db-xpbm7"
2024-05-19 17:31:38.798634 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-5567db46db-xpbm7
2024-05-19 17:31:38.798668 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:38.798684 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:31:38.798692 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-exporter-minikube-5567db46db-xpbm7"
2024-05-19 17:31:38.811367 D | ceph-nodedaemon-controller: "rook-ceph-exporter-minikube-5567db46db-xpbm7" is a ceph pod!
2024-05-19 17:31:38.811398 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-5567db46db-xpbm7"
2024-05-19 17:31:38.811404 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-5567db46db-xpbm7
2024-05-19 17:31:38.811406 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-5567db46db-xpbm7
2024-05-19 17:31:38.811409 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:38.844804 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:38.845125 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-595c5f6b68-bksvk" not found
2024-05-19 17:31:38.913523 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:38.913614 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-595c5f6b68-bksvk" not found
2024-05-19 17:31:38.953018 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-19 17:31:38.982167 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-5567db46db-xpbm7"
2024-05-19 17:31:38.982210 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-5567db46db-xpbm7
2024-05-19 17:31:38.982215 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-5567db46db-xpbm7
2024-05-19 17:31:38.982217 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:39.003354 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:39.003482 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-595c5f6b68-bksvk" not found
2024-05-19 17:31:39.065166 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-19 17:31:39.065338 D | ceph-nodedaemon-controller: reconciling node: "minikube"
2024-05-19 17:31:39.067008 D | ceph-spec: object "rook-ceph-exporter-minikube" matched on update
2024-05-19 17:31:39.067021 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:31:39.069065 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-19 17:31:39.121684 D | ceph-nodedaemon-controller: ceph exporter successfully reconciled for node "minikube". operation: "updated"
2024-05-19 17:31:39.122179 D | op-k8sutil: creating service rook-ceph-exporter
2024-05-19 17:31:39.176954 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:39.180013 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-595c5f6b68-bksvk" not found
2024-05-19 17:31:39.282917 D | op-k8sutil: updating service rook-ceph-exporter
2024-05-19 17:31:39.325226 D | ceph-nodedaemon-controller: crash collector is disabled in namespace "rook-ceph" so skipping crash retention reconcile
2024-05-19 17:31:39.352507 D | telemetry: set telemetry key: rook/kubernetes/version=v1.30.0
2024-05-19 17:31:39.359088 D | op-config: setting "rook/csi/version"="v3.11.0" option in the mon config-key store
2024-05-19 17:31:39.360148 D | exec: Running command: ceph config-key set rook/csi/version v3.11.0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:39.427688 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":2}}
2024-05-19 17:31:39.427722 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":2}}
2024-05-19 17:31:39.427829 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:889ac511-7d79-4ca4-921d-b4624254e96b ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:2 NumOsd:1 NumUpOsd:0 NumInOsd:1 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[] Version:0 NumPgs:0 DataBytes:0 UsedBytes:0 AvailableBytes:0 TotalBytes:0 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:false Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-05-19 17:31:39.427839 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2024-05-19 17:31:39.496219 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-19 17:31:39.496237 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-19 17:31:39.502619 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:39.502647 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-595c5f6b68-bksvk" not found
2024-05-19 17:31:39.960896 D | telemetry: set telemetry key: rook/csi/version=v3.11.0
2024-05-19 17:31:39.960917 D | op-config: setting "rook/cluster/mon/max-id"="0" option in the mon config-key store
2024-05-19 17:31:39.960951 D | exec: Running command: ceph config-key set rook/cluster/mon/max-id 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:40.148412 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:40.148955 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-595c5f6b68-bksvk" not found
2024-05-19 17:31:40.701161 D | telemetry: set telemetry key: rook/cluster/mon/max-id=0
2024-05-19 17:31:40.701457 D | op-config: setting "rook/cluster/mon/count"="1" option in the mon config-key store
2024-05-19 17:31:40.701684 D | exec: Running command: ceph config-key set rook/cluster/mon/count 1 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:41.339243 D | exec: Running command: ceph dashboard ac-user-create admin -i /tmp/2648630293 administrator --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:41.429619 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:41.429659 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-595c5f6b68-bksvk" not found
2024-05-19 17:31:41.492788 D | telemetry: set telemetry key: rook/cluster/mon/count=1
2024-05-19 17:31:41.492854 D | op-config: setting "rook/cluster/mon/allow-multiple-per-node"="true" option in the mon config-key store
2024-05-19 17:31:41.492886 D | exec: Running command: ceph config-key set rook/cluster/mon/allow-multiple-per-node true --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:41.836818 D | op-osd: checking osd processes status.
2024-05-19 17:31:41.840500 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:41.875172 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-5567db46db-xpbm7"
2024-05-19 17:31:41.875280 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-5567db46db-xpbm7
2024-05-19 17:31:41.875285 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-5567db46db-xpbm7
2024-05-19 17:31:41.875286 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:41.940932 I | cephclient: command failed for create dashboard user. trying again...
2024-05-19 17:31:42.278932 D | ceph-spec: update event from a CR: "rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:31:42.278959 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:42.278961 D | ceph-spec: wjkim : objNew.Name %srook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:42.278963 D | ceph-spec: wjkim : objNew.Name HasPrefix %vtrue
2024-05-19 17:31:42.279461 I | ceph-spec: Pod Status has changed for "rook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl". diff=v1.PodStatus{
	Phase: "Pending",
	Conditions: []v1.PodCondition{
		{Type: "PodReadyToStartContainers", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:37 +0000 UTC"}},
		{
			... // 3 identical fields
			LastTransitionTime: {Time: s"2024-05-19 17:31:33 +0000 UTC"},
			Reason:             "ContainersNotInitialized",
			Message: strings.Join({
				"containers with incomplete status: [",
-				"activate ",
				"expand-bluefs chown-container-data-dir]",
			}, ""),
		},
		{Type: "Ready", Status: "False", LastTransitionTime: {Time: s"2024-05-19 17:31:33 +0000 UTC"}, Reason: "ContainersNotReady", ...},
		{Type: "ContainersReady", Status: "False", LastTransitionTime: {Time: s"2024-05-19 17:31:33 +0000 UTC"}, Reason: "ContainersNotReady", ...},
		{Type: "PodScheduled", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:33 +0000 UTC"}},
	},
	Message: "",
	Reason:  "",
	... // 4 identical fields
	PodIPs:    {{IP: "10.244.0.18"}},
	StartTime: s"2024-05-19 17:31:33 +0000 UTC",
	InitContainerStatuses: []v1.ContainerStatus{
		{
			Name: "activate",
			State: v1.ContainerState{
				Waiting:    nil,
-				Running:    s"&ContainerStateRunning{StartedAt:2024-05-19 17:31:37 +0000 UTC,}",
+				Running:    nil,
-				Terminated: nil,
+				Terminated: s"&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2024-05-19 17:31:37 +0000 UTC,FinishedAt:2024-05-19 17:31:40 +0000 UTC,ContainerID:docker://05621d7afad48a5120dd42a9db40bbd8bcf5bba44d314863700facac03b081f7,}",
			},
			LastTerminationState: {},
-			Ready:                false,
+			Ready:                true,
			RestartCount:         0,
			Image:                "quay.io/ceph/ceph:v18",
			ImageID:              "docker-pullable://quay.io/ceph/ceph@sha256:6d6642f8c5b824b896ad5"...,
			ContainerID:          "docker://05621d7afad48a5120dd42a9db40bbd8bcf5bba44d314863700faca"...,
-			Started:              &true,
+			Started:              &false,
			AllocatedResources:   nil,
			Resources:            nil,
		},
		{Name: "expand-bluefs", State: {Waiting: &{Reason: "PodInitializing"}}, Image: "quay.io/ceph/ceph:v18", Started: &false, ...},
		{Name: "chown-container-data-dir", State: {Waiting: &{Reason: "PodInitializing"}}, Image: "quay.io/ceph/ceph:v18", Started: &false, ...},
	},
	ContainerStatuses: {{Name: "osd", State: {Waiting: &{Reason: "PodInitializing"}}, Image: "quay.io/ceph/ceph:v18", Started: &false, ...}},
	QOSClass:          "BestEffort",
	... // 3 identical fields
}
2024-05-19 17:31:42.279542 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:42.279599 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:42.279609 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:31:42.279632 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:31:42.547922 D | telemetry: set telemetry key: rook/cluster/mon/allow-multiple-per-node=true
2024-05-19 17:31:42.548054 D | op-config: setting "rook/cluster/mon/pvc/enabled"="false" option in the mon config-key store
2024-05-19 17:31:42.548202 D | exec: Running command: ceph config-key set rook/cluster/mon/pvc/enabled false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:42.773101 D | op-osd: validating status of osd.0
2024-05-19 17:31:42.773423 D | op-osd: osd.0 is marked 'DOWN'
2024-05-19 17:31:42.901129 D | exec: Running command: ceph orch set backend rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:43.162601 D | ceph-spec: "ceph-block-pool-controller": CephCluster resource "my-cluster" found in namespace "rook-ceph"
2024-05-19 17:31:43.162674 D | ceph-spec: "ceph-block-pool-controller": ceph status is "HEALTH_OK", operator is ready to run ceph command, reconciling
2024-05-19 17:31:43.197706 D | ceph-spec: found existing monitor secrets for cluster rook-ceph
2024-05-19 17:31:43.213407 I | ceph-spec: parsing mon endpoints: a=10.104.237.69:6789
2024-05-19 17:31:43.218455 D | ceph-spec: loaded: maxMonID=0, mons=map[a:0xc000c95710], assignment=&{Schedule:map[a:0xc000d4cb40]}
2024-05-19 17:31:43.218838 D | ceph-spec: ceph version found "18.2.2-0"
2024-05-19 17:31:43.219047 I | ceph-block-pool-controller: creating pool ".mgr" in namespace "rook-ceph"
2024-05-19 17:31:43.219061 D | exec: Running command: ceph osd crush rule create-replicated .mgr default host --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:43.862223 D | telemetry: set telemetry key: rook/cluster/mon/pvc/enabled=false
2024-05-19 17:31:43.862349 D | op-config: setting "rook/cluster/mon/stretch/enabled"="false" option in the mon config-key store
2024-05-19 17:31:43.862723 D | exec: Running command: ceph config-key set rook/cluster/mon/stretch/enabled false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:43.990055 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:43.990206 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-595c5f6b68-bksvk" not found
2024-05-19 17:31:44.109026 I | cephclient: command failed for set rook backend. trying again...
2024-05-19 17:31:44.188862 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:31:44.189602 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-5nhh7" not found
2024-05-19 17:31:44.227560 D | telemetry: set telemetry key: rook/cluster/mon/stretch/enabled=false
2024-05-19 17:31:44.227948 D | op-config: setting "rook/cluster/storage/device-set/count/total"="0" option in the mon config-key store
2024-05-19 17:31:44.229720 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/total 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:44.259993 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-19 17:31:44.261002 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:31:44.261326 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:31:44.261972 I | clusterdisruption-controller: osd "rook-ceph-osd-0" is down but no node drain is detected
2024-05-19 17:31:44.262012 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:44.653074 D | ceph-spec: update event from a CR: "rook-ceph-exporter-minikube-5567db46db-xpbm7"
2024-05-19 17:31:44.653100 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-exporter-minikube-5567db46db-xpbm7
2024-05-19 17:31:44.653103 D | ceph-spec: wjkim : objNew.Name %srook-ceph-exporter-minikube-5567db46db-xpbm7
2024-05-19 17:31:44.653104 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:31:44.754162 I | clusterdisruption-controller: osd is down in failure domain "minikube". pg health: "cluster has no PGs"
2024-05-19 17:31:44.764358 D | clusterdisruption-controller: deleting default pdb with maxUnavailable=1 for all osd
2024-05-19 17:31:44.801121 D | ceph-spec: object "rook-ceph-exporter-minikube" matched on update
2024-05-19 17:31:44.801219 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:31:44.908259 D | ceph-spec: update event from a CR: "rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:31:44.908295 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:44.908298 D | ceph-spec: wjkim : objNew.Name %srook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:44.908299 D | ceph-spec: wjkim : objNew.Name HasPrefix %vtrue
2024-05-19 17:31:44.910394 I | ceph-spec: Pod Status has changed for "rook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl". diff=v1.PodStatus{
	Phase: "Pending",
	Conditions: []v1.PodCondition{
		{Type: "PodReadyToStartContainers", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:37 +0000 UTC"}},
		{
			... // 3 identical fields
			LastTransitionTime: {Time: s"2024-05-19 17:31:33 +0000 UTC"},
			Reason:             "ContainersNotInitialized",
			Message: strings.Join({
				"containers with incomplete status: [",
-				"expand-bluefs ",
				"chown-container-data-dir]",
			}, ""),
		},
		{Type: "Ready", Status: "False", LastTransitionTime: {Time: s"2024-05-19 17:31:33 +0000 UTC"}, Reason: "ContainersNotReady", ...},
		{Type: "ContainersReady", Status: "False", LastTransitionTime: {Time: s"2024-05-19 17:31:33 +0000 UTC"}, Reason: "ContainersNotReady", ...},
		{Type: "PodScheduled", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:33 +0000 UTC"}},
	},
	Message: "",
	Reason:  "",
	... // 4 identical fields
	PodIPs:    {{IP: "10.244.0.18"}},
	StartTime: s"2024-05-19 17:31:33 +0000 UTC",
	InitContainerStatuses: []v1.ContainerStatus{
		{Name: "activate", State: {Terminated: &{Reason: "Completed", StartedAt: {Time: s"2024-05-19 17:31:37 +0000 UTC"}, FinishedAt: {Time: s"2024-05-19 17:31:40 +0000 UTC"}, ContainerID: "docker://05621d7afad48a5120dd42a9db40bbd8bcf5bba44d314863700faca"...}}, Ready: true, Image: "quay.io/ceph/ceph:v18", ...},
		{
			Name: "expand-bluefs",
			State: v1.ContainerState{
-				Waiting:    s"&ContainerStateWaiting{Reason:PodInitializing,Message:,}",
+				Waiting:    nil,
				Running:    nil,
-				Terminated: nil,
+				Terminated: s"&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2024-05-19 17:31:43 +0000 UTC,FinishedAt:2024-05-19 17:31:44 +0000 UTC,ContainerID:docker://111ede3a0ee3ff184dca46ab613179c973c4e36b5091af68ed179618645d2a14,}",
			},
			LastTerminationState: {},
-			Ready:                false,
+			Ready:                true,
			RestartCount:         0,
			Image:                "quay.io/ceph/ceph:v18",
-			ImageID:              "",
+			ImageID:              "docker-pullable://quay.io/ceph/ceph@sha256:6d6642f8c5b824b896ad5b1574e0c49b7807098bb9b946067e7bd2f03adf5b6e",
-			ContainerID:          "",
+			ContainerID:          "docker://111ede3a0ee3ff184dca46ab613179c973c4e36b5091af68ed179618645d2a14",
			Started:              &false,
			AllocatedResources:   nil,
			Resources:            nil,
		},
		{Name: "chown-container-data-dir", State: {Waiting: &{Reason: "PodInitializing"}}, Image: "quay.io/ceph/ceph:v18", Started: &false, ...},
	},
	ContainerStatuses: {{Name: "osd", State: {Waiting: &{Reason: "PodInitializing"}}, Image: "quay.io/ceph/ceph:v18", Started: &false, ...}},
	QOSClass:          "BestEffort",
	... // 3 identical fields
}
2024-05-19 17:31:44.910669 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:44.910872 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:44.910881 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:31:44.910924 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:31:45.407168 D | exec: Running command: ceph osd pool get .mgr all --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:45.437456 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/total=0
2024-05-19 17:31:45.437476 D | op-config: setting "rook/cluster/storage/device-set/count/portable"="0" option in the mon config-key store
2024-05-19 17:31:45.437526 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/portable 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:45.904507 D | exec: Running command: ceph osd pool create .mgr 0 replicated .mgr --size 1 --yes-i-really-mean-it --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:46.159863 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/portable=0
2024-05-19 17:31:46.159930 D | op-config: setting "rook/cluster/storage/device-set/count/non-portable"="0" option in the mon config-key store
2024-05-19 17:31:46.159972 D | exec: Running command: ceph config-key set rook/cluster/storage/device-set/count/non-portable 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:46.521875 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:31:46.522219 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-kwcwk" not found
2024-05-19 17:31:46.944107 E | op-mgr: failed modules: "dashboard". failed to initialize dashboard: failed to set login credentials for the ceph dashboard: failed to create dashboard user "admin": max command retries exceeded
2024-05-19 17:31:46.969445 D | telemetry: set telemetry key: rook/cluster/storage/device-set/count/non-portable=0
2024-05-19 17:31:46.969615 D | op-config: setting "rook/cluster/network/provider"="" option in the mon config-key store
2024-05-19 17:31:46.969743 D | exec: Running command: ceph config-key set rook/cluster/network/provider  --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:47.133430 D | ceph-spec: update event from a CR: "rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:31:47.133463 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:47.133466 D | ceph-spec: wjkim : objNew.Name %srook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:47.133479 D | ceph-spec: wjkim : objNew.Name HasPrefix %vtrue
2024-05-19 17:31:47.134154 I | ceph-spec: Pod Status has changed for "rook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl". diff=v1.PodStatus{
	Phase: "Pending",
	Conditions: []v1.PodCondition{
		{Type: "PodReadyToStartContainers", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:37 +0000 UTC"}},
-		{
-			Type:               "Initialized",
-			Status:             "False",
-			LastTransitionTime: s"2024-05-19 17:31:33 +0000 UTC",
-			Reason:             "ContainersNotInitialized",
-			Message:            "containers with incomplete status: [chown-container-data-dir]",
-		},
+		{
+			Type:               "Initialized",
+			Status:             "True",
+			LastTransitionTime: s"2024-05-19 17:31:47 +0000 UTC",
+		},
		{Type: "Ready", Status: "False", LastTransitionTime: {Time: s"2024-05-19 17:31:33 +0000 UTC"}, Reason: "ContainersNotReady", ...},
		{Type: "ContainersReady", Status: "False", LastTransitionTime: {Time: s"2024-05-19 17:31:33 +0000 UTC"}, Reason: "ContainersNotReady", ...},
		{Type: "PodScheduled", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:33 +0000 UTC"}},
	},
	Message: "",
	Reason:  "",
	... // 4 identical fields
	PodIPs:    {{IP: "10.244.0.18"}},
	StartTime: s"2024-05-19 17:31:33 +0000 UTC",
	InitContainerStatuses: []v1.ContainerStatus{
		{Name: "activate", State: {Terminated: &{Reason: "Completed", StartedAt: {Time: s"2024-05-19 17:31:37 +0000 UTC"}, FinishedAt: {Time: s"2024-05-19 17:31:40 +0000 UTC"}, ContainerID: "docker://05621d7afad48a5120dd42a9db40bbd8bcf5bba44d314863700faca"...}}, Ready: true, Image: "quay.io/ceph/ceph:v18", ...},
		{Name: "expand-bluefs", State: {Terminated: &{Reason: "Completed", StartedAt: {Time: s"2024-05-19 17:31:43 +0000 UTC"}, FinishedAt: {Time: s"2024-05-19 17:31:44 +0000 UTC"}, ContainerID: "docker://111ede3a0ee3ff184dca46ab613179c973c4e36b5091af68ed17961"...}}, Ready: true, Image: "quay.io/ceph/ceph:v18", ...},
		{
			Name: "chown-container-data-dir",
			State: v1.ContainerState{
-				Waiting:    s"&ContainerStateWaiting{Reason:PodInitializing,Message:,}",
+				Waiting:    nil,
				Running:    nil,
-				Terminated: nil,
+				Terminated: s"&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2024-05-19 17:31:45 +0000 UTC,FinishedAt:2024-05-19 17:31:45 +0000 UTC,ContainerID:docker://c99ab05e5f18bb2c62d42de3bfd27ecd144e3791fd1eb309bb34d7ecbb5a199f,}",
			},
			LastTerminationState: {},
-			Ready:                false,
+			Ready:                true,
			RestartCount:         0,
			Image:                "quay.io/ceph/ceph:v18",
-			ImageID:              "",
+			ImageID:              "docker-pullable://quay.io/ceph/ceph@sha256:6d6642f8c5b824b896ad5b1574e0c49b7807098bb9b946067e7bd2f03adf5b6e",
-			ContainerID:          "",
+			ContainerID:          "docker://c99ab05e5f18bb2c62d42de3bfd27ecd144e3791fd1eb309bb34d7ecbb5a199f",
			Started:              &false,
			AllocatedResources:   nil,
			Resources:            nil,
		},
	},
	ContainerStatuses: {{Name: "osd", State: {Waiting: &{Reason: "PodInitializing"}}, Image: "quay.io/ceph/ceph:v18", Started: &false, ...}},
	QOSClass:          "BestEffort",
	... // 3 identical fields
}
2024-05-19 17:31:47.134239 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:47.134303 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:47.134311 E | nvmeofosd-controller: other status: Pending
2024-05-19 17:31:47.134339 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:31:47.397192 D | exec: Running command: ceph osd pool application get .mgr --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:47.780659 D | op-osd: checking osd processes status.
2024-05-19 17:31:47.781121 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:48.358914 D | ceph-spec: update event from a CR: "rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:31:48.359384 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:48.359415 D | ceph-spec: wjkim : objNew.Name %srook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:48.359424 D | ceph-spec: wjkim : objNew.Name HasPrefix %vtrue
2024-05-19 17:31:48.359926 I | ceph-spec: Pod Status has changed for "rook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl". diff=v1.PodStatus{
-	Phase:      "Pending",
+	Phase:      "Running",
	Conditions: {{Type: "PodReadyToStartContainers", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:37 +0000 UTC"}}, {Type: "Initialized", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:47 +0000 UTC"}}, {Type: "Ready", Status: "False", LastTransitionTime: {Time: s"2024-05-19 17:31:33 +0000 UTC"}, Reason: "ContainersNotReady", ...}, {Type: "ContainersReady", Status: "False", LastTransitionTime: {Time: s"2024-05-19 17:31:33 +0000 UTC"}, Reason: "ContainersNotReady", ...}, ...},
	Message:    "",
	... // 6 identical fields
	StartTime:             s"2024-05-19 17:31:33 +0000 UTC",
	InitContainerStatuses: {{Name: "activate", State: {Terminated: &{Reason: "Completed", StartedAt: {Time: s"2024-05-19 17:31:37 +0000 UTC"}, FinishedAt: {Time: s"2024-05-19 17:31:40 +0000 UTC"}, ContainerID: "docker://05621d7afad48a5120dd42a9db40bbd8bcf5bba44d314863700faca"...}}, Ready: true, Image: "quay.io/ceph/ceph:v18", ...}, {Name: "expand-bluefs", State: {Terminated: &{Reason: "Completed", StartedAt: {Time: s"2024-05-19 17:31:43 +0000 UTC"}, FinishedAt: {Time: s"2024-05-19 17:31:44 +0000 UTC"}, ContainerID: "docker://111ede3a0ee3ff184dca46ab613179c973c4e36b5091af68ed17961"...}}, Ready: true, Image: "quay.io/ceph/ceph:v18", ...}, {Name: "chown-container-data-dir", State: {Terminated: &{Reason: "Completed", StartedAt: {Time: s"2024-05-19 17:31:45 +0000 UTC"}, FinishedAt: {Time: s"2024-05-19 17:31:45 +0000 UTC"}, ContainerID: "docker://c99ab05e5f18bb2c62d42de3bfd27ecd144e3791fd1eb309bb34d7e"...}}, Ready: true, Image: "quay.io/ceph/ceph:v18", ...}},
	ContainerStatuses: []v1.ContainerStatus{
		{
			Name: "osd",
			State: v1.ContainerState{
-				Waiting:    s"&ContainerStateWaiting{Reason:PodInitializing,Message:,}",
+				Waiting:    nil,
-				Running:    nil,
+				Running:    s"&ContainerStateRunning{StartedAt:2024-05-19 17:31:48 +0000 UTC,}",
				Terminated: nil,
			},
			LastTerminationState: {},
			Ready:                false,
			RestartCount:         0,
			Image:                "quay.io/ceph/ceph:v18",
-			ImageID:              "",
+			ImageID:              "docker-pullable://quay.io/ceph/ceph@sha256:6d6642f8c5b824b896ad5b1574e0c49b7807098bb9b946067e7bd2f03adf5b6e",
-			ContainerID:          "",
+			ContainerID:          "docker://bb96dc16b0bffdbe1a0aeb3693d4bb39974aed3085721b4c8afe94a899a9a285",
			Started:              &false,
			AllocatedResources:   nil,
			Resources:            nil,
		},
	},
	QOSClass:                   "BestEffort",
	EphemeralContainerStatuses: nil,
	... // 2 identical fields
}
2024-05-19 17:31:48.360040 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:31:48.361899 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:31:48.361959 E | nvmeofosd-controller: other status: Running
2024-05-19 17:31:48.368275 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:31:48.622938 D | telemetry: set telemetry key: rook/cluster/network/provider=
2024-05-19 17:31:48.623049 D | op-config: setting "rook/cluster/external-mode"="false" option in the mon config-key store
2024-05-19 17:31:48.623063 D | exec: Running command: ceph config-key set rook/cluster/external-mode false --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:48.626830 D | exec: Running command: ceph osd pool application enable .mgr mgr --yes-i-really-mean-it --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:48.802151 D | op-osd: validating status of osd.0
2024-05-19 17:31:48.802351 D | op-osd: osd.0 is marked 'DOWN'
2024-05-19 17:31:49.062813 D | telemetry: set telemetry key: rook/cluster/external-mode=false
2024-05-19 17:31:49.062950 I | ceph-cluster-controller: reporting node telemetry
2024-05-19 17:31:49.074940 D | op-config: setting "rook/node/count/kubernetes-total"="1" option in the mon config-key store
2024-05-19 17:31:49.075011 D | exec: Running command: ceph config-key set rook/node/count/kubernetes-total 1 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:49.113769 D | exec: Running command: ceph orch set backend rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:49.115685 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:49.117743 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-595c5f6b68-bksvk" not found
2024-05-19 17:31:49.600300 I | cephclient: reconciling replicated pool .mgr succeeded
2024-05-19 17:31:49.602447 D | cephclient: skipping check for failure domain and deviceClass on pool ".mgr" as it is not specified
2024-05-19 17:31:49.602468 I | ceph-block-pool-controller: initializing pool ".mgr" for RBD use
2024-05-19 17:31:49.602674 D | exec: Running command: rbd pool init .mgr --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2024-05-19 17:31:49.923965 D | telemetry: set telemetry key: rook/node/count/kubernetes-total=1
2024-05-19 17:31:49.923992 D | op-config: setting "rook/node/count/with-ceph-daemons"="-1" option in the mon config-key store
2024-05-19 17:31:49.924003 D | exec: Running command: ceph config-key set rook/node/count/with-ceph-daemons -1 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:49.988164 I | cephclient: command failed for set rook backend. trying again...
2024-05-19 17:31:50.035874 I | ceph-block-pool-controller: successfully initialized pool ".mgr" for RBD use
2024-05-19 17:31:50.036060 D | ceph-block-pool-controller: configuring RBD per-image IO statistics collection
2024-05-19 17:31:50.051621 D | exec: Running command: ceph config get mgr mgr/prometheus/rbd_stats_pools --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:50.419601 D | ceph-block-pool-controller: RBD per-image IO statistics will be collected for pools: []
2024-05-19 17:31:50.419667 I | op-config: setting "mgr"="mgr/prometheus/rbd_stats_pools"="" option to the mon configuration database
2024-05-19 17:31:50.419839 D | exec: Running command: ceph config set mgr mgr/prometheus/rbd_stats_pools  --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:50.454535 D | telemetry: set telemetry key: rook/node/count/with-ceph-daemons=-1
2024-05-19 17:31:50.458880 D | op-config: setting "rook/node/count/with-csi-rbd-plugin"="1" option in the mon config-key store
2024-05-19 17:31:50.458911 D | exec: Running command: ceph config-key set rook/node/count/with-csi-rbd-plugin 1 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:50.691059 I | op-config: successfully set "mgr"="mgr/prometheus/rbd_stats_pools"="" option to the mon configuration database
2024-05-19 17:31:50.691101 D | ceph-block-pool-controller: configured RBD per-image IO statistics collection
2024-05-19 17:31:50.691426 D | ceph-block-pool-controller: reconciling create rbd mirror peer configuration
2024-05-19 17:31:50.691468 D | cephclient: retrieving mirroring pool ".mgr" info
2024-05-19 17:31:50.691482 D | exec: Running command: rbd mirror pool info .mgr --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:50.773144 D | telemetry: set telemetry key: rook/node/count/with-csi-rbd-plugin=1
2024-05-19 17:31:50.778361 D | op-config: setting "rook/node/count/with-csi-cephfs-plugin"="1" option in the mon config-key store
2024-05-19 17:31:50.778554 D | exec: Running command: ceph config-key set rook/node/count/with-csi-cephfs-plugin 1 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:51.141568 D | telemetry: set telemetry key: rook/node/count/with-csi-cephfs-plugin=1
2024-05-19 17:31:51.145500 D | op-config: setting "rook/node/count/with-csi-nfs-plugin"="0" option in the mon config-key store
2024-05-19 17:31:51.145525 D | exec: Running command: ceph config-key set rook/node/count/with-csi-nfs-plugin 0 --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:51.451669 D | telemetry: set telemetry key: rook/node/count/with-csi-nfs-plugin=0
2024-05-19 17:31:53.823883 D | op-osd: checking osd processes status.
2024-05-19 17:31:53.825255 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:54.992835 D | exec: Running command: ceph orch set backend rook --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:55.526106 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-cwphf
2024-05-19 17:31:55.526330 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-cwphf" not found
2024-05-19 17:31:55.965879 D | op-osd: validating status of osd.0
2024-05-19 17:31:55.966009 D | op-osd: osd.0 is marked 'DOWN'
2024-05-19 17:31:56.588024 I | cephclient: command failed for set rook backend. trying again...
2024-05-19 17:31:59.358323 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:31:59.358538 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-595c5f6b68-bksvk" not found
2024-05-19 17:31:59.542522 I | cephclient: disabling mirroring for pool ".mgr"
2024-05-19 17:31:59.542644 D | exec: Running command: rbd mirror pool disable .mgr --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2024-05-19 17:31:59.634604 I | ceph-block-pool-controller: successfully disabled mirroring on the pool ".mgr"
2024-05-19 17:31:59.655128 D | ceph-spec: update event from a CR: "builtin-mgr"
2024-05-19 17:31:59.655148 D | ceph-spec: update event on CephBlockPool CR
2024-05-19 17:31:59.655309 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-19 17:31:59.655357 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:31:59.655397 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:31:59.655534 I | clusterdisruption-controller: osd "rook-ceph-osd-0" is down but no node drain is detected
2024-05-19 17:31:59.655549 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:31:59.656387 D | ceph-block-pool-controller: pool "rook-ceph/builtin-mgr" status updated to "Ready"
2024-05-19 17:31:59.656404 D | ceph-block-pool-controller: done reconciling
2024-05-19 17:31:59.656413 D | ceph-block-pool-controller: successfully configured CephBlockPool "rook-ceph/builtin-mgr"
2024-05-19 17:32:00.053499 I | clusterdisruption-controller: osd is down in failure domain "minikube". pg health: "cluster has no PGs"
2024-05-19 17:32:00.054141 D | clusterdisruption-controller: deleting default pdb with maxUnavailable=1 for all osd
2024-05-19 17:32:00.968344 D | op-osd: checking osd processes status.
2024-05-19 17:32:00.969495 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:32:01.474209 D | op-osd: validating status of osd.0
2024-05-19 17:32:01.474667 D | op-osd: osd.0 is healthy.
2024-05-19 17:32:01.593962 E | op-mgr: failed modules: "orchestrator modules". failed to set rook orchestrator backend: failed to set rook as the orchestrator backend: max command retries exceeded
2024-05-19 17:32:02.384296 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-19 17:32:02.388136 D | ceph-cluster-controller: node "minikube" is ready, checking if it can run OSDs
2024-05-19 17:32:02.388180 D | exec: Running command: ceph osd crush ls minikube --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:32:07.676441 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:32:07.676611 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-mon-a-canary-58bccfff95-98stx" not found
2024-05-19 17:32:07.676673 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:32:07.676680 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-kwcwk" not found
2024-05-19 17:32:07.676718 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-19 17:32:07.686904 D | op-mon: checking health of mons
2024-05-19 17:32:07.687129 D | op-mon: Acquiring lock for mon orchestration
2024-05-19 17:32:07.695607 D | op-mon: Acquired lock for mon orchestration
2024-05-19 17:32:07.698212 D | op-osd: checking osd processes status.
2024-05-19 17:32:07.698900 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:32:07.703846 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:32:07.704103 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:32:07.704448 I | clusterdisruption-controller: osd "rook-ceph-osd-0" is down but no node drain is detected
2024-05-19 17:32:07.704557 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:32:07.857594 D | op-mon: Checking health for mons in cluster "rook-ceph"
2024-05-19 17:32:07.857769 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:32:09.033543 D | ceph-spec: update event from a CR: "csi-rbdplugin-provisioner-d9b9d694c-9nsng"
2024-05-19 17:32:09.033609 D | ceph-spec: update event on Pod %qrook-ceph/csi-rbdplugin-provisioner-d9b9d694c-9nsng
2024-05-19 17:32:09.033614 D | ceph-spec: wjkim : objNew.Name %scsi-rbdplugin-provisioner-d9b9d694c-9nsng
2024-05-19 17:32:09.033633 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:32:09.307605 D | ceph-cluster-controller: node watcher: node "minikube" is already an OSD node with "[\"osd.0\"]"
2024-05-19 17:32:10.343051 D | ceph-spec: update event from a CR: "csi-cephfsplugin-provisioner-868bf46b56-7tnmw"
2024-05-19 17:32:10.343230 D | ceph-spec: update event on Pod %qrook-ceph/csi-cephfsplugin-provisioner-868bf46b56-7tnmw
2024-05-19 17:32:10.343235 D | ceph-spec: wjkim : objNew.Name %scsi-cephfsplugin-provisioner-868bf46b56-7tnmw
2024-05-19 17:32:10.343236 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:32:10.559329 D | ceph-spec: update event from a CR: "rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:32:10.559600 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:32:10.559683 D | ceph-spec: wjkim : objNew.Name %srook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:32:10.559810 D | ceph-spec: wjkim : objNew.Name HasPrefix %vtrue
2024-05-19 17:32:10.602170 I | ceph-spec: Pod Status has changed for "rook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl". diff=v1.PodStatus{
	... // 9 identical fields
	StartTime:             s"2024-05-19 17:31:33 +0000 UTC",
	InitContainerStatuses: {{Name: "activate", State: {Terminated: &{Reason: "Completed", StartedAt: {Time: s"2024-05-19 17:31:37 +0000 UTC"}, FinishedAt: {Time: s"2024-05-19 17:31:40 +0000 UTC"}, ContainerID: "docker://05621d7afad48a5120dd42a9db40bbd8bcf5bba44d314863700faca"...}}, Ready: true, Image: "quay.io/ceph/ceph:v18", ...}, {Name: "expand-bluefs", State: {Terminated: &{Reason: "Completed", StartedAt: {Time: s"2024-05-19 17:31:43 +0000 UTC"}, FinishedAt: {Time: s"2024-05-19 17:31:44 +0000 UTC"}, ContainerID: "docker://111ede3a0ee3ff184dca46ab613179c973c4e36b5091af68ed17961"...}}, Ready: true, Image: "quay.io/ceph/ceph:v18", ...}, {Name: "chown-container-data-dir", State: {Terminated: &{Reason: "Completed", StartedAt: {Time: s"2024-05-19 17:31:45 +0000 UTC"}, FinishedAt: {Time: s"2024-05-19 17:31:45 +0000 UTC"}, ContainerID: "docker://c99ab05e5f18bb2c62d42de3bfd27ecd144e3791fd1eb309bb34d7e"...}}, Ready: true, Image: "quay.io/ceph/ceph:v18", ...}},
	ContainerStatuses: []v1.ContainerStatus{
		{
			... // 6 identical fields
			ImageID:            "docker-pullable://quay.io/ceph/ceph@sha256:6d6642f8c5b824b896ad5"...,
			ContainerID:        "docker://bb96dc16b0bffdbe1a0aeb3693d4bb39974aed3085721b4c8afe94a"...,
-			Started:            &false,
+			Started:            &true,
			AllocatedResources: nil,
			Resources:          nil,
		},
	},
	QOSClass:                   "BestEffort",
	EphemeralContainerStatuses: nil,
	... // 2 identical fields
}
2024-05-19 17:32:10.602362 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:32:10.602798 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:32:10.602807 E | nvmeofosd-controller: other status: Running
2024-05-19 17:32:10.603591 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:32:10.621792 D | ceph-spec: update event from a CR: "rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:32:10.621832 D | ceph-spec: update event on Pod %qrook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:32:10.621835 D | ceph-spec: wjkim : objNew.Name %srook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:32:10.621836 D | ceph-spec: wjkim : objNew.Name HasPrefix %vtrue
2024-05-19 17:32:10.652053 D | ceph-spec: object "rook-ceph-osd-0" matched on update
2024-05-19 17:32:10.652068 D | ceph-spec: do not reconcile deployments updates
2024-05-19 17:32:10.654606 I | ceph-spec: Pod Status has changed for "rook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl". diff=v1.PodStatus{
	Phase: "Running",
	Conditions: []v1.PodCondition{
		{Type: "PodReadyToStartContainers", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:37 +0000 UTC"}},
		{Type: "Initialized", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:47 +0000 UTC"}},
-		{
-			Type:               "Ready",
-			Status:             "False",
-			LastTransitionTime: s"2024-05-19 17:31:33 +0000 UTC",
-			Reason:             "ContainersNotReady",
-			Message:            "containers with unready status: [osd]",
-		},
+		{
+			Type:               "Ready",
+			Status:             "True",
+			LastTransitionTime: s"2024-05-19 17:32:10 +0000 UTC",
+		},
-		{
-			Type:               "ContainersReady",
-			Status:             "False",
-			LastTransitionTime: s"2024-05-19 17:31:33 +0000 UTC",
-			Reason:             "ContainersNotReady",
-			Message:            "containers with unready status: [osd]",
-		},
+		{
+			Type:               "ContainersReady",
+			Status:             "True",
+			LastTransitionTime: s"2024-05-19 17:32:10 +0000 UTC",
+		},
		{Type: "PodScheduled", Status: "True", LastTransitionTime: {Time: s"2024-05-19 17:31:33 +0000 UTC"}},
	},
	Message: "",
	Reason:  "",
	... // 5 identical fields
	StartTime:             s"2024-05-19 17:31:33 +0000 UTC",
	InitContainerStatuses: {{Name: "activate", State: {Terminated: &{Reason: "Completed", StartedAt: {Time: s"2024-05-19 17:31:37 +0000 UTC"}, FinishedAt: {Time: s"2024-05-19 17:31:40 +0000 UTC"}, ContainerID: "docker://05621d7afad48a5120dd42a9db40bbd8bcf5bba44d314863700faca"...}}, Ready: true, Image: "quay.io/ceph/ceph:v18", ...}, {Name: "expand-bluefs", State: {Terminated: &{Reason: "Completed", StartedAt: {Time: s"2024-05-19 17:31:43 +0000 UTC"}, FinishedAt: {Time: s"2024-05-19 17:31:44 +0000 UTC"}, ContainerID: "docker://111ede3a0ee3ff184dca46ab613179c973c4e36b5091af68ed17961"...}}, Ready: true, Image: "quay.io/ceph/ceph:v18", ...}, {Name: "chown-container-data-dir", State: {Terminated: &{Reason: "Completed", StartedAt: {Time: s"2024-05-19 17:31:45 +0000 UTC"}, FinishedAt: {Time: s"2024-05-19 17:31:45 +0000 UTC"}, ContainerID: "docker://c99ab05e5f18bb2c62d42de3bfd27ecd144e3791fd1eb309bb34d7e"...}}, Ready: true, Image: "quay.io/ceph/ceph:v18", ...}},
	ContainerStatuses: []v1.ContainerStatus{
		{
			Name:                 "osd",
			State:                {Running: &{StartedAt: {Time: s"2024-05-19 17:31:48 +0000 UTC"}}},
			LastTerminationState: {},
-			Ready:                false,
+			Ready:                true,
			RestartCount:         0,
			Image:                "quay.io/ceph/ceph:v18",
			... // 5 identical fields
		},
	},
	QOSClass:                   "BestEffort",
	EphemeralContainerStatuses: nil,
	... // 2 identical fields
}
2024-05-19 17:32:10.654683 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-osd-0-7f5bb7dcb9-v2tzl
2024-05-19 17:32:10.654749 I | nvmeofosd-controller: Found 0 NvmeOfStorage resources in namespace rook-ceph
2024-05-19 17:32:10.654751 E | nvmeofosd-controller: other status: Running
2024-05-19 17:32:10.655547 D | nvmeofosd-controller: successfully configured Pod "rook-ceph/rook-ceph-osd-0-7f5bb7dcb9-v2tzl"
2024-05-19 17:32:10.795632 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.104.237.69:6789/0 PublicAddr:10.104.237.69:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.104.237.69:3300 Nonce:0} {Type:v1 Addr:10.104.237.69:6789 Nonce:0}]}}]}}
2024-05-19 17:32:10.795655 D | op-mon: targeting the mon count 1
2024-05-19 17:32:10.795663 D | op-mon: mon "a" found in quorum
2024-05-19 17:32:10.799502 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-05-19 17:32:10.816736 D | op-mon: skipping check for multiple mons on same node since multiple mons are allowed
2024-05-19 17:32:10.816758 D | op-mon: Released lock for mon orchestration
2024-05-19 17:32:10.816764 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "30s"
2024-05-19 17:32:10.874244 I | clusterdisruption-controller: osd is down in failure domain "minikube". pg health: "cluster has no PGs"
2024-05-19 17:32:10.874294 D | clusterdisruption-controller: deleting default pdb with maxUnavailable=1 for all osd
2024-05-19 17:32:11.070377 D | op-osd: validating status of osd.0
2024-05-19 17:32:11.070982 D | op-osd: osd.0 is healthy.
2024-05-19 17:32:11.305957 D | ceph-spec: update event from a CR: "csi-rbdplugin-provisioner-d9b9d694c-9nsng"
2024-05-19 17:32:11.306126 D | ceph-spec: update event on Pod %qrook-ceph/csi-rbdplugin-provisioner-d9b9d694c-9nsng
2024-05-19 17:32:11.306129 D | ceph-spec: wjkim : objNew.Name %scsi-rbdplugin-provisioner-d9b9d694c-9nsng
2024-05-19 17:32:11.306130 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:32:11.396093 D | ceph-spec: update event from a CR: "csi-cephfsplugin-provisioner-868bf46b56-7tnmw"
2024-05-19 17:32:11.396117 D | ceph-spec: update event on Pod %qrook-ceph/csi-cephfsplugin-provisioner-868bf46b56-7tnmw
2024-05-19 17:32:11.396120 D | ceph-spec: wjkim : objNew.Name %scsi-cephfsplugin-provisioner-868bf46b56-7tnmw
2024-05-19 17:32:11.396121 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:32:12.475335 D | ceph-spec: update event from a CR: "csi-rbdplugin-provisioner-d9b9d694c-9nsng"
2024-05-19 17:32:12.475575 D | ceph-spec: update event on Pod %qrook-ceph/csi-rbdplugin-provisioner-d9b9d694c-9nsng
2024-05-19 17:32:12.475582 D | ceph-spec: wjkim : objNew.Name %scsi-rbdplugin-provisioner-d9b9d694c-9nsng
2024-05-19 17:32:12.475583 D | ceph-spec: wjkim : objNew.Name HasPrefix %vfalse
2024-05-19 17:32:12.715203 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-5567db46db-8fmrg
2024-05-19 17:32:12.715551 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-5567db46db-8fmrg" not found
2024-05-19 17:32:14.808012 D | clusterdisruption-controller: reconciling "rook-ceph/"
2024-05-19 17:32:14.808151 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:32:14.808189 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:32:14.809227 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:32:16.074813 D | op-osd: checking osd processes status.
2024-05-19 17:32:16.076251 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:32:16.093134 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:1}]
2024-05-19 17:32:16.093197 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: [minikube]. pg health: "all PGs in cluster are clean"
2024-05-19 17:32:16.093254 I | clusterdisruption-controller: all PGs are active+clean. Restoring default OSD pdb settings
2024-05-19 17:32:16.117404 I | clusterdisruption-controller: creating the default pdb "rook-ceph-osd" with maxUnavailable=1 for all osd
2024-05-19 17:32:16.303414 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube".
2024-05-19 17:32:16.983690 D | op-osd: validating status of osd.0
2024-05-19 17:32:16.983711 D | op-osd: osd.0 is healthy.
2024-05-19 17:32:19.865066 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:32:19.867725 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-595c5f6b68-bksvk" not found
2024-05-19 17:32:21.985081 D | op-osd: checking osd processes status.
2024-05-19 17:32:21.985992 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:32:22.593296 D | op-osd: validating status of osd.0
2024-05-19 17:32:22.593386 D | op-osd: osd.0 is healthy.
2024-05-19 17:32:25.150820 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-5nhh7
2024-05-19 17:32:25.151537 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-5nhh7" not found
2024-05-19 17:32:27.594877 D | op-osd: checking osd processes status.
2024-05-19 17:32:27.596373 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:32:28.432680 D | op-osd: validating status of osd.0
2024-05-19 17:32:28.432875 D | op-osd: osd.0 is healthy.
2024-05-19 17:32:33.320556 D | ceph-cluster-controller: networkfences.csiaddons.openshift.io CRD not found, skip creating networkFence
2024-05-19 17:32:33.433990 D | op-osd: checking osd processes status.
2024-05-19 17:32:33.434575 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:32:33.920136 D | op-osd: validating status of osd.0
2024-05-19 17:32:33.920385 D | op-osd: osd.0 is healthy.
2024-05-19 17:32:36.491118 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-cwphf
2024-05-19 17:32:36.492263 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-cwphf" not found
2024-05-19 17:32:38.921276 D | op-osd: checking osd processes status.
2024-05-19 17:32:38.923806 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:32:39.466296 D | op-osd: validating status of osd.0
2024-05-19 17:32:39.467057 D | op-osd: osd.0 is healthy.
2024-05-19 17:32:39.500952 D | ceph-cluster-controller: checking health of cluster
2024-05-19 17:32:39.502112 D | exec: Running command: ceph status --format json --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring
2024-05-19 17:32:39.988767 D | ceph-cluster-controller: cluster status: {Health:{Status:HEALTH_OK Checks:map[]} FSID:889ac511-7d79-4ca4-921d-b4624254e96b ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:10 NumOsd:1 NumUpOsd:1 NumInOsd:1 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:28536832 AvailableBytes:10708881408 TotalBytes:10737418240 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}
2024-05-19 17:32:40.009343 D | exec: Running command: ceph versions --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:32:40.457880 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3}}
2024-05-19 17:32:40.457924 D | cephclient: {"mon":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"mgr":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"osd":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":1},"overall":{"ceph version 18.2.2 (531c0d11a1c5d39fbfe6aa8a521f023abf3bf3e2) reef (stable)":3}}
2024-05-19 17:32:40.458493 D | ceph-cluster-controller: updating ceph cluster "rook-ceph" status and condition to &{Health:{Status:HEALTH_OK Checks:map[]} FSID:889ac511-7d79-4ca4-921d-b4624254e96b ElectionEpoch:3 Quorum:[0] QuorumNames:[a] MonMap:{Epoch:1 NumMons:1 FSID: CreatedTime: ModifiedTime: Mons:[]} OsdMap:{Epoch:10 NumOsd:1 NumUpOsd:1 NumInOsd:1 Full:false NearFull:false NumRemappedPgs:0} PgMap:{PgsByState:[{StateName:active+clean Count:1}] Version:0 NumPgs:1 DataBytes:590368 UsedBytes:28536832 AvailableBytes:10708881408 TotalBytes:10737418240 ReadBps:0 WriteBps:0 ReadOps:0 WriteOps:0 RecoveryBps:0 RecoveryObjectsPerSec:0 RecoveryKeysPerSec:0 CacheFlushBps:0 CacheEvictBps:0 CachePromoteBps:0} MgrMap:{Epoch:0 ActiveGID:0 ActiveName: ActiveAddr: Available:true Standbys:[]} Fsmap:{Epoch:1 ID:0 Up:0 In:0 Max:0 ByRank:[] UpStandby:0}}, True, ClusterCreated, Cluster created successfully
2024-05-19 17:32:40.458664 D | ceph-spec: CephCluster "rook-ceph" status: "Ready". "Cluster created successfully"
2024-05-19 17:32:40.555023 D | ceph-spec: found 1 ceph clusters in namespace "rook-ceph"
2024-05-19 17:32:40.555050 D | ceph-cluster-controller: update event on CephCluster CR
2024-05-19 17:32:40.817890 D | op-mon: checking health of mons
2024-05-19 17:32:40.817910 D | op-mon: Acquiring lock for mon orchestration
2024-05-19 17:32:40.817913 D | op-mon: Acquired lock for mon orchestration
2024-05-19 17:32:40.830312 D | op-mon: Checking health for mons in cluster "rook-ceph"
2024-05-19 17:32:40.834507 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:32:40.901660 D | clusterdisruption-controller: reconciling "rook-ceph/my-cluster"
2024-05-19 17:32:40.902926 D | clusterdisruption-controller: Using default maintenance timeout: 30m0s
2024-05-19 17:32:40.909265 D | clusterdisruption-controller: could not match failure domain. defaulting to "host"
2024-05-19 17:32:40.912823 D | exec: Running command: ceph status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:32:42.106664 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.104.237.69:6789/0 PublicAddr:10.104.237.69:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.104.237.69:3300 Nonce:0} {Type:v1 Addr:10.104.237.69:6789 Nonce:0}]}}]}}
2024-05-19 17:32:42.106700 D | op-mon: targeting the mon count 1
2024-05-19 17:32:42.106709 D | op-mon: mon "a" found in quorum
2024-05-19 17:32:42.106711 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-05-19 17:32:42.122997 D | op-mon: Released lock for mon orchestration
2024-05-19 17:32:42.123209 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "30s"
2024-05-19 17:32:42.202990 D | cephclient: all placement groups have reached a clean state: [{StateName:active+clean Count:1}]
2024-05-19 17:32:42.203048 D | clusterdisruption-controller: no OSD is down in the "host" failure domains: [minikube]. pg health: "all PGs in cluster are clean"
2024-05-19 17:32:42.203384 D | clusterdisruption-controller: deleted temporary blocking pdb for "host" failure domain "minikube".
2024-05-19 17:32:44.469575 D | op-osd: checking osd processes status.
2024-05-19 17:32:44.471352 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:32:44.771126 D | op-osd: validating status of osd.0
2024-05-19 17:32:44.771449 D | op-osd: osd.0 is healthy.
2024-05-19 17:32:48.639920 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-csi-detect-version-kwcwk
2024-05-19 17:32:48.640770 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-csi-detect-version-kwcwk" not found
2024-05-19 17:32:48.926234 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-detect-version-2lnnf
2024-05-19 17:32:48.926530 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-detect-version-2lnnf" not found
2024-05-19 17:32:49.772316 D | op-osd: checking osd processes status.
2024-05-19 17:32:49.772881 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:32:50.207592 D | op-osd: validating status of osd.0
2024-05-19 17:32:50.207846 D | op-osd: osd.0 is healthy.
2024-05-19 17:32:55.209168 D | op-osd: checking osd processes status.
2024-05-19 17:32:55.210403 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:32:55.617512 D | op-osd: validating status of osd.0
2024-05-19 17:32:55.617576 D | op-osd: osd.0 is healthy.
2024-05-19 17:33:00.618549 D | op-osd: checking osd processes status.
2024-05-19 17:33:00.618803 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:33:00.829502 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-exporter-minikube-595c5f6b68-bksvk
2024-05-19 17:33:00.829765 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-exporter-minikube-595c5f6b68-bksvk" not found
2024-05-19 17:33:00.931051 D | op-osd: validating status of osd.0
2024-05-19 17:33:00.931204 D | op-osd: osd.0 is healthy.
2024-05-19 17:33:05.934724 D | op-osd: checking osd processes status.
2024-05-19 17:33:05.935290 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:33:06.386695 D | op-osd: validating status of osd.0
2024-05-19 17:33:06.391541 D | op-osd: osd.0 is healthy.
2024-05-19 17:33:11.394901 D | op-osd: checking osd processes status.
2024-05-19 17:33:11.396276 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:33:11.713144 D | op-osd: validating status of osd.0
2024-05-19 17:33:11.713242 D | op-osd: osd.0 is healthy.
2024-05-19 17:33:12.125229 D | op-mon: checking health of mons
2024-05-19 17:33:12.125699 D | op-mon: Acquiring lock for mon orchestration
2024-05-19 17:33:12.125715 D | op-mon: Acquired lock for mon orchestration
2024-05-19 17:33:12.157076 D | op-mon: Checking health for mons in cluster "rook-ceph"
2024-05-19 17:33:12.157179 D | exec: Running command: ceph quorum_status --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:33:12.543014 D | op-mon: Mon quorum status: {Quorum:[0] MonMap:{Mons:[{Name:a Rank:0 Address:10.104.237.69:6789/0 PublicAddr:10.104.237.69:6789/0 PublicAddrs:{Addrvec:[{Type:v2 Addr:10.104.237.69:3300 Nonce:0} {Type:v1 Addr:10.104.237.69:6789 Nonce:0}]}}]}}
2024-05-19 17:33:12.543058 D | op-mon: targeting the mon count 1
2024-05-19 17:33:12.543074 D | op-mon: mon "a" found in quorum
2024-05-19 17:33:12.543076 D | op-mon: mon cluster is healthy, removing any existing canary deployment
2024-05-19 17:33:12.549238 D | op-mon: Released lock for mon orchestration
2024-05-19 17:33:12.549290 D | op-mon: ceph mon status in namespace "rook-ceph" check interval "30s"
2024-05-19 17:33:16.715826 D | op-osd: checking osd processes status.
2024-05-19 17:33:16.721839 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:33:17.243346 D | op-osd: validating status of osd.0
2024-05-19 17:33:17.243564 D | op-osd: osd.0 is healthy.
2024-05-19 17:33:22.244278 D | op-osd: checking osd processes status.
2024-05-19 17:33:22.250387 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:33:23.879930 D | op-osd: validating status of osd.0
2024-05-19 17:33:23.880784 D | op-osd: osd.0 is healthy.
2024-05-19 17:33:28.881891 D | op-osd: checking osd processes status.
2024-05-19 17:33:28.882284 D | exec: Running command: ceph osd dump --connect-timeout=15 --cluster=rook-ceph --conf=/var/lib/rook/rook-ceph/rook-ceph.config --name=client.admin --keyring=/var/lib/rook/rook-ceph/client.admin.keyring --format json
2024-05-19 17:33:29.597949 D | nvmeofosd-controller: reconciling NvmeOfOSD. Request.Namespace: rook-ceph, Request.Name: rook-ceph-mon-a-canary-58bccfff95-98stx
2024-05-19 17:33:29.598282 E | nvmeofosd-controller: `unable to fetch Pod`, Pod "rook-ceph-mon-a-canary-58bccfff95-98stx" not found
